{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62cb2cf",
   "metadata": {},
   "source": [
    "# PROJECT -  A2Z CUSTOMER SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060c14e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Data herein presented pertains to a fictional insurance company in Portugal, A2Z Insurance. The goal is to develop a customer segmentation strategy that will enable the Marketing Department to better understand the different customers' profiles and develop adequate marketing strategies. <br>\n",
    "This project is done within the cope of the **Data Mining** curricular unit of the Master's Degree in **Data Science and Advanced Analytics**.\n",
    "\n",
    "#### Group elements:\n",
    "* Ivan Jure ParaÄ‡ (20210689)\n",
    "* Nuno de Bourbon e Carvalho Melo (20210681)\n",
    "* Stuart Gallina Ottersen (20210703)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data exploration](#data-exploration)\n",
    "2. [Data preprocessing](#data-preprocessing)\n",
    "    1. [First steps](#preprocessing-first-steps)\n",
    "    2. [Dealing with outliers](#preprocessing-outliers)\n",
    "    3. [Handling missing values](#preprocessing-missing-values)\n",
    "    4. [Feature creation and cross field validation](#preprocessing-transform-validate)\n",
    "    5. [Feature selection](#preprocessing-feature-selection)\n",
    "    6. [Feature skewness](#preprocessing-scaling)\n",
    "3. [Clustering](#data-clustering)\n",
    "    1. [Sociodemographic clustering](#clustering-sociodemographic)\n",
    "    2. [Value clustering](#clustering-value)\n",
    "    3. [Product clustering](#clustering-product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832a0d1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2><center>BOILERPLATE</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d408b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line of code to install package required for KPrototypes\n",
    "# !pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be438b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import major libraries/modules\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# others\n",
    "from math import ceil\n",
    "from regressors import stats\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import chi2_contingency\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, AffinityPropagation, OPTICS, MeanShift\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4952e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de61fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SAS file with the insurance company data\n",
    "df, meta = pyreadstat.read_sas7bdat('a2z_insurance.sas7bdat')\n",
    "\n",
    "# save copy of the original dataframe\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31d92",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-exploration\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA EXPLORATION</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17472222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows and columns\n",
    "print(\"Number of observations:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])\n",
    "print(\"Features:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "df.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7724d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-preprocessing\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA PREPROCESSING</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da107c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple(df, features, plot_type, fig_dimensions = [15, 8], nr_rows = 1):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, a list of features, features,\n",
    "    a plot type, plot_type, the dimensions of the figure,\n",
    "    fig_dimensions, and the number of rows of the figure,\n",
    "    nr_rows, as argument, and plots the specified plot type\n",
    "    for the features passed as arguments.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "     features (list): features from df to plot (min. 2 features)\n",
    "     plot_type (str): type of plot (box, violin, or histogram)\n",
    "     fig_dimensions (list, tuple): figure dimensions\n",
    "     nr_rows (int): number of rows in the figure\n",
    "\n",
    "    Returns:\n",
    "     None\n",
    "    '''\n",
    "\n",
    "    sns.set(style = \"ticks\")\n",
    "\n",
    "    fig, axes = plt.subplots(nr_rows, ceil(len(features) / nr_rows), figsize = fig_dimensions)\n",
    "    \n",
    "    # iterate through axes and associate each plot to one\n",
    "    for ax, feat in zip(axes.flatten(), features):\n",
    "        if (plot_type.lower() == \"box\") or (plot_type.lower() == \"boxplot\"):\n",
    "            sns.boxplot(\n",
    "                data = df,\n",
    "                x = feat,\n",
    "                color = \"C9\",\n",
    "                ax = ax\n",
    "            )\n",
    "\n",
    "        if (plot_type.lower() == \"violin\" or (plot_type.lower() == \"violinplot\")):\n",
    "            sns.violinplot(\n",
    "                data = df,\n",
    "                x = feat,\n",
    "                color = \"C9\",\n",
    "                ax = ax\n",
    "            )\n",
    "\n",
    "        if (plot_type.lower() == \"histogram\" or (plot_type.lower() == \"histplot\")):\n",
    "            sns.histplot(\n",
    "                data = df,\n",
    "                x = feat,\n",
    "                color = \"C9\",\n",
    "                ax = ax\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3b73b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-first-steps\"></a>\n",
    "\n",
    "<h3><right>First steps</right></h3>\n",
    "\n",
    "* Set customer ID as index\n",
    "* Remove duplicated observations\n",
    "* Encode EducDeg and change to float\n",
    "* Swap incoherent birth and first policy years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set customer ID as index\n",
    "df.CustID = df.CustID.astype(\"int\")\n",
    "df.set_index(\"CustID\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicated rows\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# remove duplicate rows\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(\"Removing duplicates...\")\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# store initial number of rows (after duplicate removal)\n",
    "initial_len = len(df)\n",
    "\n",
    "# check number of rows and columns again\n",
    "print(\"\\nCurrent number of observations:\", df.shape[0])\n",
    "print(\"Current number of features:\", df.shape[1])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data types\n",
    "\n",
    "# extract number from EducDeg, save as float\n",
    "df.EducDeg = df.EducDeg.str.extract(\"(\\d+)\").astype(\"float\")\n",
    "# education degree mapper (to replace numbers when needed)\n",
    "educ_mapper = {1: \"Basic\", 2: \"High School\", 3: \"BSc/MSc\", 4: \"PhD\"}\n",
    "\n",
    "# check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swaps years if policy seems to be made before birth\n",
    "nr_incoherences = len(df[df.BirthYear > df.FirstPolYear])\n",
    "pc_incoherences = round(nr_incoherences/len(df)*100, 1)\n",
    "\n",
    "print(f\"Number of people with a policy before birth: {nr_incoherences} \"\n",
    "    f\"({pc_incoherences}% of the dataset)\")\n",
    "\n",
    "# such high number of inconsistencies suggests systematic error\n",
    "# assumption: in these cases BirthYear and FirstPolYear were introduced in the wrong fields\n",
    "print(\"Swapping birth and first policy years...\")\n",
    "\n",
    "# swap FirstPolYear and BirthYear values when birth occurs after first policy creation\n",
    "row_filter = df.BirthYear > df.FirstPolYear\n",
    "df.loc[row_filter, [\"FirstPolYear\", \"BirthYear\"]] = df.loc[row_filter, [\"BirthYear\", \"FirstPolYear\"]].values\n",
    "\n",
    "# check if the years were correctly swapped\n",
    "nr_incoherences = len(df[df.BirthYear > df.FirstPolYear])\n",
    "print(f\"Number of people with a policy before birth: {nr_incoherences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7f8fd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-outliers\"></a>\n",
    "\n",
    "<h3><right>Dealing with outliers</right></h3>\n",
    "\n",
    "* Remove outliers and store them in a separate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, filters):\n",
    "\n",
    "    # create a separate dataframe for the outliers\n",
    "    outliers = pd.DataFrame()\n",
    "\n",
    "    # remove outliers from main dataframe\n",
    "    # add outliers to a separate dataframe\n",
    "    for filter_ in filters:\n",
    "        outliers = outliers.append(df[filter_])\n",
    "        df = df[~filter_]\n",
    "        \n",
    "    # determine number of outliers removed\n",
    "    n_outliers = len(outliers)\n",
    "    pc_removed = round(n_outliers/initial_len*100, 2)\n",
    "    print(f\"Number of outliers removed: {n_outliers} ({pc_removed}% of all observations)\")\n",
    "\n",
    "    return (df, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03192d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\"]\n",
    "\n",
    "# boxplots of metric features\n",
    "plot_multiple(df, metric_features, \"box\", [20, 10], nr_rows = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BirthYear 1028 assumed to be a typo\n",
    "# 0 and 9 are fairly close in a qwerty keyboard, replaced with 1928\n",
    "df.loc[df.BirthYear == 1028, \"BirthYear\"] = 1928\n",
    "\n",
    "# remove outliers from df, store them in df_outliers1\n",
    "filters = (\n",
    "    (df.FirstPolYear.ge(2017)),\n",
    "    (df.MonthSal.ge(20000)),\n",
    "    (df.CustMonVal.le(-2000)),\n",
    "    (df.CustMonVal.ge(1250)),\n",
    "    (df.ClaimsRate.ge(4)),\n",
    "    (df.PremMotor.ge(3000)),\n",
    "    (df.PremHousehold.ge(1160)),\n",
    "    (df.PremHealth.ge(405)),\n",
    "    (df.PremLife.ge(290)),\n",
    "    (df.PremWork.ge(275))\n",
    ")\n",
    "\n",
    "df, df_outliers1 = remove_outliers(df, filters)\n",
    "\n",
    "# boxplots of metric features after removing outliers\n",
    "plot_multiple(df, metric_features, \"box\", [20, 10], nr_rows = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625bc27",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-missing-values\"></a>\n",
    "\n",
    "<h3><right>Handling missing values</right></h3>\n",
    "\n",
    "* Check feature and row completeness (in df and df_outliers)\n",
    "* Remove customers with no information about Premiums\n",
    "* Remove customers with missing FirstPolYear or BirthYear\n",
    "* Remove customers with missing EducDeg\n",
    "* Fill Premium missing values with zero\n",
    "* Create Linear Regression model to impute MonthSal\n",
    "* Create Logistic Regression model to impute Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(df):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, as argument and\n",
    "    computes the number/percentage of missing\n",
    "    values per feature, and the number of rows\n",
    "    with the maximum number of missing values.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "\n",
    "    Returns:\n",
    "     None\n",
    "    '''\n",
    "\n",
    "    # number and percentage of NaN values per feature\n",
    "    nr_nans = df.isna().sum()\n",
    "    pc_nans = df.isna().mean()*100\n",
    "    feature_nans = pd.concat([nr_nans, pc_nans], axis = 1)\n",
    "    feature_nans.rename(columns = {0: \"nr\", 1: \"%\"}, inplace = True)\n",
    "\n",
    "    # show number of missing values per feature\n",
    "    print(\"Missing values per feature:\\n\", feature_nans)\n",
    "\n",
    "    # check row completeness\n",
    "    # max number of NaN values in a row and number of rows with that many NaN\n",
    "    max_row_nan = df.isnull().sum(axis = 1).max()\n",
    "    \n",
    "    print(f\"\\nMaximum number of NaN values per row: {max_row_nan} \"\n",
    "        f\"({len(df[df.isnull().sum(axis = 1) == max_row_nan])} rows)\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd056b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(df, cols, max_nan_per_row):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, a list of columns, cols,\n",
    "    and the maximum number of nan per row, max_nan_per_row,\n",
    "    and removes rows that have missing values in the columns\n",
    "    passed on as arguments as well as rows that exceed the\n",
    "    maximum number of missing values allowed.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "     cols (list): column(s) in df to remove NaN\n",
    "     max_nan_per_row (int): max number of NaN allowed per row\n",
    "\n",
    "    Returns:\n",
    "     df (dataframe): dataframe after removing NaN\n",
    "     df_nan (dataframe): dataframe with the NaN rows removed from df\n",
    "    '''\n",
    "\n",
    "    # create dataframe to store rows discarded due to missing values\n",
    "    df_nan = pd.DataFrame()\n",
    "\n",
    "    # removing rows above the threshold of maximum missing values allowed\n",
    "    max_nan_rows = df[df.isnull().sum(axis = 1) >= max_nan_per_row]\n",
    "    df_nan = df_nan.append(max_nan_rows)\n",
    "    df.drop(max_nan_rows.index, inplace = True)\n",
    "\n",
    "    print(\n",
    "        f\"Removed {len(max_nan_rows)} row(s) \"\n",
    "        f\"missing ({round(max_nan_per_row/len(df.columns)*100, 1)}% of data)\"\n",
    "    )\n",
    "\n",
    "    # remove rows with missing values from features passed as arguments\n",
    "    # add these rows to df_nan\n",
    "    for col in cols:\n",
    "        nan_ = df[col].isna()\n",
    "        nr_removed = sum(nan_)\n",
    "\n",
    "        df_nan = df_nan.append(df[nan_])\n",
    "        df = df[~nan_]\n",
    "\n",
    "        print(f\"Removed {nr_removed} row(s) (NaN in {col})\")\n",
    "\n",
    "    # show total number of missing values removed\n",
    "    nr_rows_removed = len(df_nan)\n",
    "    total_removed = initial_len - len(df)\n",
    "\n",
    "    print(\n",
    "        f\"Total number of rows removed: {nr_rows_removed}\\n\"\n",
    "        f\"Total number of rows removed so far: {total_removed} \"\n",
    "        f\"({round(total_removed/initial_len*100, 2)}%)\"\n",
    "    )\n",
    "\n",
    "    return (df, df_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_imputer(df, cols):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, and a list of columns,\n",
    "    cols, and replaces existing missing values in\n",
    "    those columns with zero.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "     cols (list): column(s) in df to impute NaN with 0\n",
    "    \n",
    "    Returns:\n",
    "     df (dataframe): dataframe after value imputation\n",
    "    '''\n",
    "\n",
    "    # save the total number of imputed values\n",
    "    total_imputations = 0\n",
    "\n",
    "    # impute each NaN value in cols with zero\n",
    "    for col in cols:\n",
    "        col_nan = sum(df[col].isna())\n",
    "        df[col].fillna(0, inplace = True)\n",
    "\n",
    "        # print number of imputed values per column\n",
    "        print(f\"Imputed {col_nan} out of {col_nan} missing value(s) in {col}\")\n",
    "        \n",
    "        total_imputations += col_nan\n",
    "\n",
    "    print(f\"Total number of imputations: {total_imputations}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_imputer(df, regressors, regressand):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, a list of regressor names,\n",
    "    regressors, and the name of the regressand, regressand,\n",
    "    and uses a linear regression model to predict missing\n",
    "    values in the regressand based on the regressors.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "     regressors (str, list): list of columns in df to be used\n",
    "    in linear regression to predict the regressand\n",
    "     regressand (str): name of column in df whose missing values\n",
    "    will be imputed with values predicted by linear regression\n",
    "\n",
    "    Returns:\n",
    "     df (dataframe): dataframe after value imputation\n",
    "    '''\n",
    "\n",
    "    # independent, X, and dependent, y, variables\n",
    "    X = df.dropna()[regressors]\n",
    "    y = df.dropna()[regressand]\n",
    "\n",
    "    # split train and validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 15)\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if (type(regressors) is not list) and (type(regressors) is str):\n",
    "        X_train = np.array(X_train).reshape(-1, 1)\n",
    "        X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "    # scale features using MinMaxScaler() with parameters from X_train\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    # scale the training and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # create and fit model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict salary of the validation set\n",
    "    y_pred = lin_reg.predict(X_val_scaled)\n",
    "\n",
    "    # compute metrics for the predictions made\n",
    "    mse = metrics.mean_squared_error(y_val, y_pred)\n",
    "    rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n",
    "    mae = metrics.mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"======================================================\")\n",
    "    print(\"                Linear regression model               \")\n",
    "    print(\"======================================================\")\n",
    "    print(\"Mean square error:\", round(mse, 2))\n",
    "    print(\"Root mean square error:\", round(rmse, 2))\n",
    "    print(\"Mean absolute error:\", round(mae, 2))\n",
    "    stats.summary(clf = lin_reg, X = X_train_scaled, y = y_train)\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    # predict missing values in the regressand column\n",
    "    X_test = df.loc[df[regressand].isna(), regressors]\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_test = np.array(X_test).reshape(-1, 1)\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "    # impute missing values in the regressand column\n",
    "    df.loc[df[regressand].isna(), regressand] = y_pred\n",
    "    \n",
    "    print(f\"\\nImputed {len(y_pred)} values in {regressand}\")\n",
    "\n",
    "    #df.loc[df.MonthSal.isna(), \"MonthSal\"] = y_pred\n",
    "\n",
    "    # multiple linear regression model to impute missing MonthSal values\n",
    "    # use all features but the MonthSal to train the model\n",
    "\n",
    "    # define independent and dependent variables\n",
    "    #X = df.dropna().drop([\"MonthSal\"], axis = 1)\n",
    "    #y = df.dropna().MonthSal\n",
    "\n",
    "    # split train and test data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)\n",
    "\n",
    "    # scale train and test data\n",
    "    #scaler = MinMaxScaler().fit(X_train)\n",
    "    #X_train_scaled = scaler.transform(X_train)\n",
    "    #X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # create and fit model\n",
    "    #lin_model = LinearRegression()\n",
    "    #lin_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict y\n",
    "    #y_pred = lin_model.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the predictions of the linear reg model\n",
    "    #xlabels = X_train.columns\n",
    "    #stats.summary(clf = lin_model, X = X_train_scaled, y = y_train, xlabels = xlabels)\n",
    "    #mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    #rmse = metrics.mean_squared_error(y_test, y_pred, squared = False)\n",
    "    #mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    #print(mse)\n",
    "    #print(rmse)\n",
    "    #print(mae)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_imputer(df, regressors, regressand):\n",
    "            \n",
    "    # imputing missing Children values\n",
    "    # conclusion based on previous feature selection: use only BirthYear\n",
    "\n",
    "    # independent, X, and dependent, y, variables\n",
    "    X = df.dropna()[regressors]\n",
    "    y = df.dropna()[regressand]\n",
    "\n",
    "    # split data into train (70%) and validation (30%) datasets\n",
    "    # 70% have children, 30% dont, decided to stratify\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_train = np.array(X_train).reshape(-1, 1)\n",
    "        X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "    # scale features using MinMaxScaler with parameters from X_train\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    # scale the training and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # create a logistic regression model\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict y\n",
    "    y_pred = log_reg.predict(X_val_scaled)\n",
    "\n",
    "    # evaluate the predictions of the logistic reg model\n",
    "    conf_matrix = metrics.confusion_matrix(y_val, y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix)\n",
    "    accuracy = round(metrics.accuracy_score(y_val, y_pred)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_val, y_pred)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_val, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_val, y_pred)*100, 2)\n",
    "\n",
    "    print(\"==============================================\")\n",
    "    print(\"           Logistic regression model          \")\n",
    "    print(\"==============================================\")\n",
    "    print(\"Accuracy:\", accuracy, \"%\")\n",
    "    print(\"Precision:\", precision, \"%\")\n",
    "    print(\"Recall:\", recall, \"%\")\n",
    "    print(\"F1 score:\", f1, \"%\")\n",
    "    print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    # predict missing values in the regressand column\n",
    "    X_test = df.loc[df[regressand].isna(), regressors]\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_test = np.array(X_test).reshape(-1, 1)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "    # impute missing values in the regressand column\n",
    "    df.loc[df[regressand].isna(), regressand] = y_pred\n",
    "\n",
    "    print(f\"\\nImputed {len(y_pred)} values in {regressand}\")\n",
    "    \n",
    "    # attempting children prediction using KNN\n",
    "    # overall results were worse than with logistic regression\n",
    "\n",
    "    #data = df.dropna().drop([\"Children\", \"GeoLivArea\"], axis = 1)\n",
    "    #target = df.dropna().Children\n",
    "\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(data, target, train_size=0.70, stratify = target, random_state=5)\n",
    "\n",
    "    #modelKNN = KNeighborsClassifier()\n",
    "    #modelKNN.fit(X = X_train, y = y_train)\n",
    "    #labels_train = modelKNN.predict(X_train)\n",
    "    #labels_val = modelKNN.predict(X_val)\n",
    "\n",
    "    #conf_matrix = metrics.confusion_matrix(y_val, labels_val)\n",
    "    #accuracy = round(metrics.accuracy_score(y_val, labels_val)*100, 2)\n",
    "    #precision = round(metrics.precision_score(y_val, labels_val)*100, 2)\n",
    "    #recall = round(metrics.recall_score(y_val, labels_val)*100, 2)\n",
    "    #f1 = round(metrics.f1_score(y_val, labels_val)*100, 2)\n",
    "\n",
    "    #print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "    #print(\"Accuracy:\", accuracy, \"%\")\n",
    "    #print(\"Precision:\", precision, \"%\")\n",
    "    #print(\"Recall:\", recall, \"%\")\n",
    "    #print(\"F1 score:\", f1, \"%\") \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_feat_selection(df, regressand):\n",
    "\n",
    "    # independent, X, and dependent, y, variables\n",
    "    X = df.dropna().drop(columns = regressand)\n",
    "    y = df.dropna()[regressand]\n",
    "\n",
    "    # split data into train (70%) and validation (30%) datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "    # scale features using MinMaxScaler with parameters from X_train\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    # scale the training set\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    # scale the test set\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # recursive feature elimination\n",
    "    numfeats_list = np.arange(1, len(df.columns))\n",
    "    scores = {}\n",
    "\n",
    "    for n in range(len(numfeats_list)):\n",
    "        log_reg = LogisticRegression()\n",
    "        rfe = RFE(log_reg, numfeats_list[n])\n",
    "        \n",
    "        X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val_scaled)\n",
    "        log_reg.fit(X_train_rfe, y_train)\n",
    "        \n",
    "        score = log_reg.score(X_val_rfe, y_val)\n",
    "        scores[n+1] = score\n",
    "\n",
    "    # RFE selected 1 single feature\n",
    "    best_num_feats = max(scores, key = scores.get)\n",
    "    rfe = RFE(estimator = log_reg, n_features_to_select = best_num_feats)\n",
    "    rfe.fit(X = X_train_scaled, y = y_train)\n",
    "    selected_features = pd.Series(rfe.support_, index = X_train.columns, name = \"RFE\")\n",
    "\n",
    "    # compute correlation between Children and other features\n",
    "    correlations = pd.Series(df.corr()[regressand], name = \"Correlation\")\n",
    "\n",
    "    # compute Lasso coefficients\n",
    "    reg = LassoCV()\n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "    lasso_coef = pd.Series(reg.coef_, index = X_train.columns, name = \"Lasso\")\n",
    "\n",
    "    # concatenate features selected by rfe, correlations, and lasso coefficients\n",
    "    selection_df = pd.concat([selected_features, correlations, lasso_coef], axis = 1).drop(regressand)\n",
    "\n",
    "    # plot correlation and lasso coefficients\n",
    "    coef_names = [\"Correlation\", \"Lasso\"]\n",
    "\n",
    "    sns.set(font_scale = 1.4)\n",
    "    sns.set_style(\"white\")\n",
    "    fig, axes = plt.subplots(1, ceil(len(coef_names)), figsize = (24, 10))\n",
    "\n",
    "    for ax, coef in zip(axes.flatten(), coef_names):\n",
    "        sns.barplot(data = selection_df,\n",
    "                    x = coef,\n",
    "                    y = selection_df.index,\n",
    "                    hue = \"RFE\",\n",
    "                    palette = [\"darkgray\", \"C9\"],\n",
    "                    order = selection_df.sort_values(coef).index,\n",
    "                    ax = ax)\n",
    "        ax.axvline(x = 0, linestyle = \"--\", color = \"darkgray\", label = \"_nolegend_\")\n",
    "        ax.set_xlabel(coef + \" coefficient\", fontsize = 16)\n",
    "        ax.legend(title = \"RFE\", loc = \"upper right\", fontsize = 14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in the dataframe\n",
    "check_completeness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values in the specified features\n",
    "max_nan_per_row = df.isnull().sum(axis = 1).max()\n",
    "cols = [\"FirstPolYear\", \"BirthYear\", \"EducDeg\"]\n",
    "df, df_nan = remove_missing_values(df, cols, max_nan_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45537541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in the Premium features\n",
    "cols = [col for col in df if col.startswith(\"Prem\")]\n",
    "df = zero_imputer(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature(s) to use for linear regression of MonthSal\n",
    "corr_salary = pd.Series(df.corr().MonthSal, name = \"Correlation\").sort_values()\n",
    "print(\"Correlation between salary and other features:\")\n",
    "print(round(corr_salary, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing MonthSal values based on linear regression\n",
    "salary_regressors = \"BirthYear\"\n",
    "regressand = \"MonthSal\"\n",
    "df = linreg_imputer(df, salary_regressors, regressand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa50a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose features used for the logistic regression of Children\n",
    "# compute and plot RFE results, correlations, and Lasso coefficients\n",
    "logreg_feat_selection(df, \"Children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daef62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute children missing values based on selected features\n",
    "children_regressors = \"BirthYear\"\n",
    "df = logreg_imputer(df, children_regressors, \"Children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all NaN values were dealt with\n",
    "check_completeness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa751e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in df_outliers1\n",
    "check_completeness(df_outliers1)\n",
    "\n",
    "# deal with Premium missing values in df_outliers1\n",
    "cols = [\"PremMotor\", \"PremHealth\", \"PremLife\", \"PremWork\"]\n",
    "df_outliers1 = zero_imputer(df_outliers1, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999be855",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-transform-validate\"></a>\n",
    "\n",
    "<h3><right>Feature creation</right></h3>\n",
    "\n",
    "* Add columns: Age, FirstPolAge, CustYears, Generation, PremTotal, Premium ratios\n",
    "* Check if EducDeg makes sense given the customer's age\n",
    "* Convert MonthSal to YearSal\n",
    "* Remove outliers from newly created features\n",
    "* Transform skewed metric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creator(df):\n",
    "    \n",
    "    # save columns originally present in the dataframe\n",
    "    initial_cols = df.columns\n",
    "\n",
    "    # create Age column\n",
    "    # (customers age as of 2016)\n",
    "    curr_year = 2016\n",
    "    cust_ages = curr_year - df.BirthYear\n",
    "    df[\"Age\"] = cust_ages\n",
    "    \n",
    "    # create FirstPolAge column\n",
    "    # (customers age at the time of the first policy creation)\n",
    "    firstpol_ages = df.FirstPolYear - df.BirthYear\n",
    "    df[\"FirstPolAge\"] = firstpol_ages\n",
    "    \n",
    "    # create CustYears column\n",
    "    # (number of years as a customer to the company)\n",
    "    df[\"CustYears\"] = curr_year - df.FirstPolYear\n",
    "    \n",
    "    # create Generation column\n",
    "    # (generation customers belongs to)\n",
    "    df.loc[(df.BirthYear >= 1928) & (df.BirthYear <= 1945), \"Generation\"] = 1 # Silent Gen\n",
    "    df.loc[(df.BirthYear >= 1946) & (df.BirthYear <= 1964), \"Generation\"] = 2 # Baby Boomer\n",
    "    df.loc[(df.BirthYear >= 1965) & (df.BirthYear <= 1980), \"Generation\"] = 3 # Gen X\n",
    "    df.loc[(df.BirthYear >= 1981) & (df.BirthYear <= 1996), \"Generation\"] = 4 # Millennial\n",
    "    df.loc[(df.BirthYear >= 1997) & (df.BirthYear <= 2012), \"Generation\"] = 5 # Gen Z\n",
    "    df.Generation = df.Generation.astype(\"float\")\n",
    "    \n",
    "    # create YearSal column\n",
    "    # (customers yearly income)\n",
    "    year_sals = df.MonthSal * 12\n",
    "    df[\"YearSal\"] = year_sals\n",
    "    \n",
    "    # create a PremTotal column\n",
    "    # (customers total money spent on premiums)\n",
    "    premium_cols = [\"PremMotor\", \"PremHousehold\", \"PremHealth\", \"PremLife\", \"PremWork\"]\n",
    "    df[\"PremTotal\"] = df[premium_cols].sum(axis = 1)\n",
    "\n",
    "    # create Effort column\n",
    "    # (proportion of the yearly salary each customer spends on the company)\n",
    "    efforts = df.PremTotal / df.YearSal\n",
    "    df[\"EffortRatio\"] = efforts\n",
    "    \n",
    "    # create PremiumRatio columns\n",
    "    # (proportion of the total premium spent on each type of policy)\n",
    "    for col in premium_cols:\n",
    "        newcol_vals = df[col]/df[\"PremTotal\"]\n",
    "        newcol_name = col + \"Ratio\"\n",
    "        df[newcol_name] = newcol_vals\n",
    "\n",
    "    # verbose: list of newly created columns\n",
    "    new_cols = df.columns.drop(initial_cols)\n",
    "    print(f\"Created {len(new_cols)} new features:\\n{list(new_cols)}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataframe with new features\n",
    "df = feature_creator(df)\n",
    "\n",
    "# also add the new features to df_outliers1\n",
    "# needed so that they can be assigned to clusters later on\n",
    "df_outliers1 = feature_creator(df_outliers1)\n",
    "\n",
    "# update metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violinplots of the newly created year-related features\n",
    "features = [\"Age\", \"FirstPolAge\", \"CustYears\"]\n",
    "plot_multiple(df, features, \"violin\", fig_dimensions = [16, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Generation feature\n",
    "# generation mapper\n",
    "gen_mapper = {\n",
    "    1: \"Silent Gen\",\n",
    "    2: \"Baby Boomer\",\n",
    "    3: \"Gen X\",\n",
    "    4: \"Millennial\",\n",
    "    5: \"Gen Z\"\n",
    "}\n",
    "\n",
    "# count number of customers per generation\n",
    "gen_count = df.groupby(\"Generation\").size().sort_values(ascending = False)\n",
    "gen_count.rename(index = gen_mapper, inplace = True)\n",
    "\n",
    "# visualize number of customers per generation\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "sns.barplot(x = gen_count.index, y = gen_count.values, order = gen_count.index)\n",
    "plt.title(\"Number of customers per generation\", fontsize = 22)\n",
    "plt.xlabel(\"Generation\", fontsize = 18)\n",
    "plt.ylabel(\"Count\", fontsize = 18)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# almost no customers from the younger generation, Gen Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate EducDeg and Age\n",
    "# minimum age is 18 - meaning everyone can have an education up to High School\n",
    "# minimum age of 20 for a BSc\n",
    "# minimum age of 23 for a PhD (skipping MSc and finishing in 3 years, UK or outside of the EU)\n",
    "educdeg_min_age = df.groupby(\"EducDeg\").Age.min().rename(index = educ_mapper)\n",
    "print(\"======================================\")\n",
    "print(\"   Cross-validating EducDeg and Age   \")\n",
    "print(\"======================================\")\n",
    "print(\"Minimum age associated to each EducDeg\")\n",
    "print(educdeg_min_age)\n",
    "\n",
    "# no incoherences found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metric features\n",
    "plot_multiple(df, metric_features, \"box\", fig_dimensions = [25, 15], nr_rows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions to remove outliers\n",
    "filters = (\n",
    "    (df.PremHouseholdRatio.ge(0.85)),\n",
    "    (df.PremHealthRatio.ge(0.64)),\n",
    "    (df.PremLifeRatio.ge(0.40)),\n",
    "    (df.PremWorkRatio.ge(0.40)),\n",
    "    (df.PremWorkRatio.le(-0.05)),\n",
    "    (df.PremTotal.le(350)),\n",
    "    (df.PremTotal.ge(1750)),\n",
    "    (df.EffortRatio.ge(0.26))\n",
    ")\n",
    "\n",
    "# remove outliers from newly created features, store them in outliers2\n",
    "df, df_outliers2 = remove_outliers(df, filters)\n",
    "\n",
    "# merge the two outliers dataframes\n",
    "df_outliers = pd.concat([df_outliers1, df_outliers2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32249b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of metric features after removing outliers\n",
    "plot_multiple(df, metric_features, \"box\", [25, 15], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880bbd4",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-feature-selection\"></a>\n",
    "\n",
    "<h3><right>Feature selection</right></h3>\n",
    "\n",
    "* Remove redundant features\n",
    "* Remove irrelevant features\n",
    "* Prepare dataframe for cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a09ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(df, method = \"pearson\"):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, and a coefficient, method,\n",
    "    as arguments and creates a correlation heatmap showing\n",
    "    the correlations between all pairs of features in the\n",
    "    dataframe.\n",
    "    '''\n",
    "\n",
    "    sns.set(style = \"ticks\")\n",
    "    plt.subplots(figsize = (20, 15))\n",
    "\n",
    "    mask = np.triu(np.ones_like(df.corr(method = method), dtype = bool))\n",
    "    corr_heatmap = sns.heatmap(df.corr(method = method), mask = mask, vmin = -1, vmax = 1, annot = True, cmap = 'BrBG')\n",
    "    corr_heatmap.set_title('Triangle Correlation Heatmap', fontdict = {'fontsize': 14}, pad = 8)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_df(df):\n",
    "    \n",
    "    # reorganise column order\n",
    "    df = df.loc[:, [\n",
    "        \"Generation\",\n",
    "        \"Age\",\n",
    "        \"YearSal\",\n",
    "        \"EducDeg\",\n",
    "        \"Children\",\n",
    "        \"FirstPolAge\",\n",
    "        \"CustYears\",\n",
    "        \"CustMonVal\",\n",
    "        \"ClaimsRate\",\n",
    "        \"PremMotor\",\n",
    "        \"PremMotorRatio\",\n",
    "        \"PremHousehold\",\n",
    "        \"PremHouseholdRatio\",\n",
    "        \"PremHealth\",\n",
    "        \"PremHealthRatio\",\n",
    "        \"PremLife\",\n",
    "        \"PremLifeRatio\",\n",
    "        \"PremWork\",\n",
    "        \"PremWorkRatio\",\n",
    "        \"PremTotal\",\n",
    "        \"EffortRatio\"\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # rename a few columns\n",
    "    df = df.rename(columns = {\n",
    "        \"CustMonVal\": \"CMV\",\n",
    "        \"PremMotor\": \"Motor\",\n",
    "        \"PremMotorRatio\": \"MotorRatio\",\n",
    "        \"PremHousehold\": \"House\",\n",
    "        \"PremHouseholdRatio\": \"HouseRatio\",\n",
    "        \"PremHealth\": \"Health\",\n",
    "        \"PremHealthRatio\": \"HealthRatio\",\n",
    "        \"PremLife\": \"Life\",\n",
    "        \"PremLifeRatio\": \"LifeRatio\",\n",
    "        \"PremWork\": \"Work\",\n",
    "        \"PremWorkRatio\": \"WorkRatio\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap of features in the dataframe\n",
    "corr_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant features from main dataframe (perfect correlation)\n",
    "df.drop(columns = [\"FirstPolYear\", \"BirthYear\", \"MonthSal\"], inplace = True)\n",
    "\n",
    "# TO BE DECIDED (based on future clustering) !!!!!!!!!!!!!!\n",
    "# Age vs FirstPolAge vs Generation vs YearSal\n",
    "# ClaimsRate vs CustMonVal\n",
    "# PremTotal vs PremHousehold\n",
    "# PremTotal vs PremHousehold_ratio\n",
    "# Prem ratios vs Prem values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap after updating features\n",
    "corr_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"]\n",
    "\n",
    "print(\"Metric features:\", metric_features)\n",
    "print(\"Non-metric features:\", non_metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap hinted at GeoLivArea having no meaningful correlation with any feature\n",
    "# visual representation of GeoLivArea and its relation with other features\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 3, figsize=(20, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x = df[\"GeoLivArea\"], y = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# confirms that GeoLivArea appears to have no discriminative power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove GeoLivArea from the main dataframe\n",
    "df.drop(columns = \"GeoLivArea\", inplace = True)\n",
    "\n",
    "# remove GeoLivArea from df_outliers\n",
    "df_outliers.drop(columns = \"GeoLivArea\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure and visualize the main dataframe before cluster analysis\n",
    "df = restructure_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure and visualize the outliers dataframe before cluster analysis\n",
    "df_outliers = restructure_df(df_outliers)\n",
    "df_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"Children\", \"Generation\"]\n",
    "\n",
    "# compute total number of observations removed\n",
    "nr_rows_removed = initial_len - len(df)\n",
    "pc_rows_removed = round((nr_rows_removed / initial_len) * 100, 2)\n",
    "\n",
    "print(\n",
    "    f\"Total number of observations removed: {nr_rows_removed} ({pc_rows_removed}%)\\n\"\n",
    "    f\"Final number of observations: {df.shape[0]} ({100 - pc_rows_removed}%)\\n\"\n",
    "    f\"Current number of features: {df.shape[1]}\\n\"\n",
    "    \"Features:\",\n",
    "    list(df.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3394a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-scaling\"></a>\n",
    "\n",
    "<h3><right>Feature skewness</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_skewed_features(df, features, transformation):\n",
    "\n",
    "    # create dataframe with the transformed features\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    for feat in features:\n",
    "        # ensure all values are non-negative and apply square root transformation\n",
    "        if transformation.lower() == \"sqrt\":\n",
    "            df_transformed[feat] = np.sqrt(df_transformed[feat] + abs(df_transformed[feat].min()))\n",
    "\n",
    "        # ensure all values are positive and apply log10 transformation\n",
    "        if transformation.lower() == \"log10\":\n",
    "            df_transformed[feat] = np.log10(df_transformed[feat] + abs(df_transformed[feat].min()) + 1)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"CMV\",\n",
    "    \"House\",\n",
    "    \"HouseRatio\",\n",
    "    \"Health\",\n",
    "    \"HealthRatio\",\n",
    "    \"Life\",\n",
    "    \"LifeRatio\",\n",
    "    \"Work\",\n",
    "    \"WorkRatio\",\n",
    "    \"PremTotal\",\n",
    "    \"EffortRatio\"\n",
    "]\n",
    "\n",
    "# plot histogram of the potentially skewed metric features\n",
    "plot_multiple(df, features, \"histogram\", [25, 15], nr_rows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of skewed metric features\n",
    "skewed_features = [\n",
    "    \"House\",\n",
    "    \"Life\",\n",
    "    \"LifeRatio\",\n",
    "    \"Work\",\n",
    "    \"WorkRatio\",\n",
    "    \"PremTotal\",\n",
    "    \"EffortRatio\"\n",
    "]\n",
    "\n",
    "# apply transformation to the skewed features  (could maybe apply log10 to PremTotal alone???????????????????????)\n",
    "# save transformed features (and the others) in a copy of the original dataframe to preserve original values\n",
    "df_transformed = transform_skewed_features(df, skewed_features, transformation = \"sqrt\")\n",
    "\n",
    "# plot histogram of transformed features\n",
    "plot_multiple(df_transformed, features, \"histogram\", [25, 15], nr_rows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features in the dataframe\n",
    "features = list(df_transformed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7517422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point there are 3 important dataframes\n",
    "#  df             - original, not transformed values (no square root transformation of skewed features)\n",
    "#  df_transformed - identical to df except that square root transformation was applied to skewed metric features\n",
    "#  df_outliers    - all of the outliers removed from df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabde1da",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-clustering\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>CLUSTERING</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, scaler_name):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, and the name of the scaler,\n",
    "    scaler_name, as arguments and applies the corresponding\n",
    "    scaling method to the dataframe.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe without the target, y\n",
    "     scaler_name (str): name of the scaler (minmax,\n",
    "    standard, robust)\n",
    "\n",
    "    Returns:\n",
    "     df_scaled (dataframe): scaled version of the original df\n",
    "    '''\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "    features = list(df.columns)\n",
    "\n",
    "    if scaler_name.lower() == \"minmax\":\n",
    "        df_scaled[features] = MinMaxScaler().fit_transform(df)\n",
    "\n",
    "    if scaler_name.lower() == \"standard\":\n",
    "        df_scaled[features] = StandardScaler().fit_transform(df)\n",
    "\n",
    "    if scaler_name.lower() == \"robust\":\n",
    "        df_scaled[features] = RobustScaler().fit_transform(df)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd882a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(df, k):\n",
    "\n",
    "    '''\n",
    "    Takes in a dataframe, df, and a number of clusters, k,\n",
    "    and assigns rows of the dataframe to clusters based on\n",
    "    the KMeans algorithm.\n",
    "\n",
    "    Arguments:\n",
    "     df (dataframe): dataframe\n",
    "     k (int): number of clusters\n",
    "\n",
    "    Returns:\n",
    "     centroids (ndarray): center of each cluster\n",
    "     labels (ndarray): label of each row indicating the\n",
    "    cluster it was assigned to\n",
    "    '''\n",
    "    \n",
    "    clusters = KMeans(\n",
    "        n_clusters = k,\n",
    "        random_state = 15\n",
    "    ).fit(df)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centers_\n",
    "\n",
    "    return (centroids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kprototypes_clustering(df, k, cat_index, init = \"Huang\"):\n",
    "\n",
    "    clusters = KPrototypes(\n",
    "        n_clusters = k,\n",
    "        init = init,\n",
    "        random_state = 15\n",
    "    ).fit(df, categorical = cat_index)\n",
    "    labels = clusters.labels_\n",
    "    centroids = clusters.cluster_centroids_\n",
    "\n",
    "    return (centroids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f50e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_plot(df, nmax_clusters, algorithm, cat_index = None, init = \"Huang\"):\n",
    "    \n",
    "    # store inertia values in a list\n",
    "    y_vals = []\n",
    "    \n",
    "    # determine inertia for each number of clusters\n",
    "    for n in np.arange(2, nmax_clusters + 1):\n",
    "        if algorithm.lower() == \"kmeans\":\n",
    "            km_clusters = KMeans(n_clusters = n, random_state = 15).fit(df)\n",
    "            inertia = km_clusters.inertia_\n",
    "            y_vals.append(inertia)\n",
    "\n",
    "        if algorithm.lower() == \"kprototypes\":\n",
    "            if type(cat_index) is int:\n",
    "                cat_index = [cat_index]\n",
    "            kp_clusters = KPrototypes(\n",
    "                n_clusters = n,\n",
    "                init = init,\n",
    "                random_state = 15\n",
    "            ).fit(df, categorical = cat_index)\n",
    "            cost = kp_clusters.cost_\n",
    "            y_vals.append(cost)\n",
    "    \n",
    "    # plot elbow graph\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.subplots(figsize = (8, 6))\n",
    "    sns.lineplot(x = np.arange(2, nmax_clusters + 1),\n",
    "        y = y_vals,\n",
    "        color = \"k\",\n",
    "        marker = 'o',\n",
    "        mew = 0,\n",
    "        linewidth = 3)\n",
    "    plt.title(\"Elbow plot\", fontsize = 22)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlim([0, nmax_clusters+1])\n",
    "    plt.xlabel(\"Number of clusters, k\", fontsize = 18)\n",
    "\n",
    "    if algorithm.lower() == \"kmeans\":\n",
    "        plt.ylabel(\"Inertia\", fontsize = 18)\n",
    "    elif algorithm.lower() == \"kprototypes\":\n",
    "        plt.ylabel(\"Cost\", fontsize = 18)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ba825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(df, n_clusters = None, threshold = 10000, affinity = \"euclidean\", linkage = \"ward\"):\n",
    "    \n",
    "    # determine clusters\n",
    "    clusters = AgglomerativeClustering(\n",
    "        n_clusters = n_clusters,\n",
    "        affinity = affinity,\n",
    "        linkage = linkage,\n",
    "        distance_threshold = threshold\n",
    "    ).fit(df)\n",
    "    \n",
    "    # retrieve cluster labels and distances\n",
    "    labels = clusters.labels_\n",
    "    distances = clusters.distances_\n",
    "    \n",
    "    counts = np.zeros(clusters.children_.shape[0])\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    for i, merge in enumerate(clusters.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack([clusters.children_, distances, counts]).astype(float)\n",
    "    \n",
    "    return (distances, labels, linkage_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b76369",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering-sociodemographic\"></a>\n",
    "\n",
    "<h3><right>Sociodemographic clustering</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sociodemographic features - Generation, Age, YearSal, EducDeg, Children\n",
    "# Age redundant with YearSal and Generation - choose which of these to include in the clustering\n",
    "# do not use a distance based method if including Children (binary)\n",
    "df_sociodem = df[[\"Age\", \"Generation\", \"YearSal\",\"EducDeg\", \"Children\"]]\n",
    "\n",
    "# data scaling\n",
    "df_sociodem_minmax = data_scaler(df_sociodem, \"minmax\")\n",
    "df_sociodem_standard = data_scaler(df_sociodem.drop(columns = \"Children\"), \"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering solution 1: KMeans (Age, EducDeg)\n",
    "km1_sociodem = df_sociodem_minmax[[\"Age\", \"EducDeg\"]]\n",
    "\n",
    "elbow_plot(km1_sociodem, 15, \"kmeans\")\n",
    "\n",
    "km1_sociodem_centroids, km1_sociodem_clusters = kmeans_clustering(km1_sociodem, 4)\n",
    "\n",
    "df[\"Cluster\"] = km1_sociodem_clusters\n",
    "\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd85829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering solution 2: KMeans followed by Hierarchical clustering (Age, EducDeg)\n",
    "km2_sociodem_centroids, km2_sociodem_clusters = kmeans_clustering(km1_sociodem, 15)\n",
    "\n",
    "km2_sociodem_linkage = hierarchy.linkage(km2_sociodem_centroids, method = \"ward\")\n",
    "hierarchy.dendrogram(km2_sociodem_linkage, color_threshold = 0.6)\n",
    "\n",
    "# dendrogram suggests 4 clusters\n",
    "df[\"Cluster\"] = km2_sociodem_clusters\n",
    "\n",
    "cluster0 = (\n",
    "    (df.Cluster == 8)\n",
    "    | (df.Cluster == 3)\n",
    "    | (df.Cluster == 11)\n",
    ")\n",
    "cluster1 = (\n",
    "    (df.Cluster == 2)\n",
    "    | (df.Cluster == 13)\n",
    "    | (df.Cluster == 6)\n",
    "    | (df.Cluster == 7)\n",
    ")\n",
    "\n",
    "cluster2 = (\n",
    "    (df.Cluster == 10)\n",
    "    | (df.Cluster == 5)\n",
    "    | (df.Cluster == 0)\n",
    "    | (df.Cluster == 12)\n",
    ")\n",
    "\n",
    "cluster3 = (\n",
    "    (df.Cluster == 1)\n",
    "    | (df.Cluster == 14)\n",
    "    | (df.Cluster == 4)\n",
    "    | (df.Cluster == 9)\n",
    ")\n",
    "\n",
    "df.loc[cluster0, \"Cluster\"] = 0\n",
    "df.loc[cluster1, \"Cluster\"] = 1\n",
    "df.loc[cluster2, \"Cluster\"] = 2\n",
    "df.loc[cluster3, \"Cluster\"] = 3\n",
    "\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1_sociodem = df_sociodem_minmax[[\"Age\", \"EducDeg\", \"Children\"]]\n",
    "\n",
    "elbow_plot(kp1_sociodem, 15, \"kprototypes\", cat_index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering solution 3: KPrototypes (Age, EducDeg, Children)\n",
    "kp1_sociodem_centroids, kp1_sociodem_clusters = kprototypes_clustering(kp1_sociodem, k = 4, cat_index = 2)\n",
    "\n",
    "df[\"Cluster\"] = kp1_sociodem_clusters\n",
    "\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering solution 4: KPrototypes followed by Hierarchical clustering (Age, EducDeg, Children)\n",
    "kp2_sociodem_centroids, kp2_sociodem_clusters = kprototypes_clustering(kp1_sociodem, k = 15, cat_index = 2)\n",
    "\n",
    "kp2_sociodem_linkage = hierarchy.linkage(kp2_sociodem_centroids, method = \"ward\")\n",
    "hierarchy.dendrogram(kp2_sociodem_linkage, color_threshold = 1.0)\n",
    "\n",
    "df[\"Cluster\"] = kp2_sociodem_clusters\n",
    "\n",
    "cluster0 = (\n",
    "    (df.Cluster == 10)\n",
    "    | (df.Cluster == 4)\n",
    "    | (df.Cluster == 2)\n",
    "    | (df.Cluster == 6)\n",
    ")\n",
    "cluster1 = (\n",
    "    (df.Cluster == 3)\n",
    "    | (df.Cluster == 12)\n",
    "    | (df.Cluster == 9)\n",
    "    | (df.Cluster == 11)\n",
    ")\n",
    "\n",
    "cluster2 = (\n",
    "    (df.Cluster == 14)\n",
    "    | (df.Cluster == 7)\n",
    "    | (df.Cluster == 8)\n",
    ")\n",
    "\n",
    "cluster3 = (\n",
    "    (df.Cluster == 1)\n",
    "    | (df.Cluster == 13)\n",
    "    | (df.Cluster == 0)\n",
    "    | (df.Cluster == 5)\n",
    ")\n",
    "\n",
    "df.loc[cluster0, \"Cluster\"] = 0\n",
    "df.loc[cluster1, \"Cluster\"] = 1\n",
    "df.loc[cluster2, \"Cluster\"] = 2\n",
    "df.loc[cluster3, \"Cluster\"] = 3\n",
    "\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1c46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3772717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip plot for cluster visualization?\n",
    "# sns.set_style(\"ticks\")\n",
    "\n",
    "# # test = df.copy()\n",
    "# # test.EducDeg = test.EducDeg.replace(educ_mapper)\n",
    "\n",
    "# x_vars = [\"Age\", \"EducDeg\"]\n",
    "\n",
    "# g = sns.PairGrid(df.sort_values(\"Age\", ascending = False),\n",
    "#                  x_vars = x_vars, y_vars = [\"Cluster\"],\n",
    "#                  hue = \"Children\",\n",
    "#                  height = 8,\n",
    "#                  aspect = 0.85)\n",
    "\n",
    "# g.map(sns.stripplot, size = 12, orient = \"h\", jitter = True,\n",
    "#       palette = [\"gray\", \"palevioletred\"], alpha = 0.1)\n",
    "\n",
    "# # add better labels\n",
    "# g.set(ylabel = \"Clusters\")\n",
    "\n",
    "# titles = [\"Age\", \"EducDeg\"]\n",
    "\n",
    "# for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "#     # set axis titles\n",
    "#     ax.set(title = title)\n",
    "\n",
    "#     # horizontal grid\n",
    "#     ax.xaxis.grid(False)\n",
    "#     ax.yaxis.grid(True, lw = 1.5)\n",
    "    \n",
    "# plt.legend(title = \"Children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8fb49",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering-value\"></a>\n",
    "\n",
    "<h3><right>Value clustering</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially relevant features for customer segmentation based on value\n",
    "df_value = df_transformed[[\n",
    "    \"FirstPolAge\",\n",
    "    \"CustYears\",\n",
    "    \"CMV\",\n",
    "    \"ClaimsRate\",\n",
    "    \"PremTotal\",\n",
    "    \"EffortRatio\"\n",
    "]]\n",
    "\n",
    "# data scaling\n",
    "df_value_minmax = data_scaler(df_value, \"minmax\")\n",
    "df_value_standard = data_scaler(df_value, \"standard\")\n",
    "df_value_robust = data_scaler(df_value, \"robust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMV and ClaimsRate largely redundant so use only one\n",
    "# not sure if EffortRatio, FirstPolAge and CustYears are useful / helpful, need to test\n",
    "# definitely include CMV or ClaimsRate + PremTotal\n",
    "# try different scaling methods: standard/robust better to deal with outliers than minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e100cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow plot\n",
    "elbow_plot(df_value_minmax, 15, \"kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72892b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering solution 1: KMeans (all features in df_values)\n",
    "km1_value = df_value_minmax[[\"CMV\", \"PremTotal\", \"EffortRatio\"]]\n",
    "\n",
    "km1_value_centroids, km1_value_clusters = kmeans_clustering(km1_value, 4)\n",
    "\n",
    "df[\"Cluster\"] = km1_value_clusters\n",
    "\n",
    "print(df.groupby(\"Cluster\").size())\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"CMV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5b57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48978ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a517ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5513f969",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering-product\"></a>\n",
    "\n",
    "<h3><right>Product clustering</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    hierarchy.dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53990751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially relevant features to perform customer segmentation based on product\n",
    "df_product = df_transformed[[\n",
    "    \"Motor\", \"MotorRatio\",\n",
    "    \"House\", \"HouseRatio\",\n",
    "    \"Health\", \"HealthRatio\",\n",
    "    \"Life\", \"LifeRatio\",\n",
    "    \"Work\", \"WorkRatio\"\n",
    "]]\n",
    "\n",
    "# data scaling\n",
    "df_product_minmax = data_scaler(df_product, \"minmax\")\n",
    "df_product_standard = data_scaler(df_product, \"standard\")\n",
    "df_product_robust = data_scaler(df_product, \"robust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aae7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try clustering approaches using the values or using the ratios, choose one of the two\n",
    "# possible good approaches: Agglomerative Clustering, KMeans, DBScan, Self Organizing Maps, others?\n",
    "# try different scaling methods: standard/robust better to deal with outliers than minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38fdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6a090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8ca91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
