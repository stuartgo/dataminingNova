{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62cb2cf",
   "metadata": {},
   "source": [
    "# PROJECT -  A2Z CUSTOMER SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060c14e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Data herein presented pertains to a fictional insurance company in Portugal, A2Z Insurance. The goal is to develop a customer segmentation that will enable the Marketing Department to better understand the different customers' profiles and develop adequate marketing strategies. <br>\n",
    "This project is done within the cope of the **Data Mining** curricular unit of the Master's Degree in **Data Science and Advanced Analytics**.\n",
    "\n",
    "#### Group elements:\n",
    "* Ivan Jure ParaÄ‡ (20210689)\n",
    "* Nuno de Bourbon e Carvalho Melo (20210681)\n",
    "* Stuart Gallina Ottersen (20210703)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "* [Data exploration](#data-exploration)\n",
    "* [Data preprocessing](#data-preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832a0d1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2><center>BOILERPLATE</center></h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d408b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line of code to install package required for KPrototypes\n",
    "# !pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be438b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import major libraries/modules\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# others\n",
    "from math import ceil\n",
    "from regressors import stats\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import chi2_contingency\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, AffinityPropagation, OPTICS, MeanShift\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4952e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de61fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SAS file with the insurance company data\n",
    "df, meta = pyreadstat.read_sas7bdat('a2z_insurance.sas7bdat')\n",
    "\n",
    "# save copy of the original dataframe\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31d92",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-exploration\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA EXPLORATION</center></h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b87964",
   "metadata": {},
   "source": [
    "**Section overview**\n",
    "* First look at the dataset.\n",
    "* Setting customer ID number as index.\n",
    "* Removal of duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17472222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows and columns\n",
    "print(\"Number of observations:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])\n",
    "print(\"Features:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "df.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set customer ID as index\n",
    "df.CustID = df.CustID.astype(\"int\")\n",
    "df.set_index(\"CustID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicated rows\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# remove duplicate rows\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(\"Removing duplicates...\")\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows and columns again\n",
    "print(\"Number of observations:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7724d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-preprocessing\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA PREPROCESSING</center></h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3b73b",
   "metadata": {},
   "source": [
    "### Checking data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data types\n",
    "\n",
    "# extract number from EducDeg, save as float\n",
    "df.EducDeg = df.EducDeg.str.extract(\"(\\d+)\").astype(\"float\")\n",
    "# education degree mapper\n",
    "educ_mapper = {1: \"Basic\", 2: \"High School\", 3: \"BSc/MSc\", 4: \"PhD\"}\n",
    "\n",
    "# check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7f8fd",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\"]\n",
    "\n",
    "# boxplot of metric features\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize = (20, 11))\n",
    "\n",
    "# iterate through axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BirthYear 1028 assumed to be a typo\n",
    "# 0 and 9 are fairly close in a qwerty keyboard, replaced with 1928\n",
    "df.loc[df.BirthYear == 1028, \"BirthYear\"] = 1928\n",
    "\n",
    "# conditions to remove outliers\n",
    "filters = ((df.FirstPolYear.ge(2017)),\n",
    "           (df.MonthSal.ge(20000)),\n",
    "           (df.CustMonVal.le(-2000)),\n",
    "           (df.CustMonVal.ge(1500)),\n",
    "           (df.ClaimsRate.ge(4)),\n",
    "           (df.PremMotor.ge(3000)),\n",
    "           (df.PremHousehold.ge(1600)),\n",
    "           (df.PremHealth.ge(5000)),\n",
    "           (df.PremWork.ge(300)))\n",
    "\n",
    "# number of observations before outliers removal\n",
    "len_df = len(df)\n",
    "\n",
    "# remove outliers from main dataframe\n",
    "# create a separate dataframe for the outliers\n",
    "df_outliers = pd.DataFrame()\n",
    "\n",
    "for filter_ in filters:\n",
    "    df_outliers = df_outliers.append(df[filter_])\n",
    "    df = df[~filter_]\n",
    "    \n",
    "# determine number of outliers removed\n",
    "n_outliers = len(df_outliers)\n",
    "pc_removed = round(n_outliers/len_df*100, 2)\n",
    "print(f\"Number of outliers removed: {n_outliers} ({pc_removed}% of all observations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots of metric features after removing outliers\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new look at the data after removing outliers\n",
    "df.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625bc27",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb663b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature completeness\n",
    "# number and percentage of NaN values per feature\n",
    "nr_nans = df.isna().sum()\n",
    "pc_nans = df.isna().mean()*100\n",
    "feature_nans = pd.concat([nr_nans, pc_nans], axis = 1)\n",
    "feature_nans.rename(columns = {0: \"nr\", 1: \"%\"}, inplace = True)\n",
    "\n",
    "# not enough missing values is a single feature to merit its exclusion\n",
    "print(\"Missing values per feature:\\n\", feature_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check row completeness\n",
    "# max number of NaN values in a row and number of rows with that many NaN\n",
    "max_row_nan = df.isnull().sum(axis = 1).max()\n",
    "print(f\"\\nMaximum number of NaN values per row: {max_row_nan} \"\n",
    "      f\"({len(df[df.isnull().sum(axis = 1) == max_row_nan])} observations)\")\n",
    "\n",
    "# a row with 4 missing values has ~30% of its data missing (13 features)\n",
    "# inspecting these rows\n",
    "max_nan_rows = df[df.isnull().sum(axis = 1) == max_row_nan]\n",
    "display(max_nan_rows)\n",
    "\n",
    "# removing these rows - no information about Premiums\n",
    "df.drop(max_nan_rows.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers dataframe for NaN values\n",
    "outliers_nan_before = df_outliers.isna().sum()\n",
    "\n",
    "# only 3 NaN - 1 PremMotor, 1 PremHealth, 1 PremLife\n",
    "# assuming no info about premiums means no premium is paid\n",
    "df_outliers.PremMotor.fillna(0, inplace = True)\n",
    "df_outliers.PremHealth.fillna(0, inplace = True)\n",
    "df_outliers.PremLife.fillna(0, inplace = True)\n",
    "\n",
    "# check if NaN values were correctly imputed\n",
    "outliers_nan_after = df_outliers.isna().sum()\n",
    "outliers_nan = pd.concat([outliers_nan_before, outliers_nan_after], axis = 1)\n",
    "outliers_nan.rename(columns = {0: \"before\", 1: \"after\"}, inplace = True)\n",
    "\n",
    "print(\"Missing values in the outliers' dataframe:\")\n",
    "outliers_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing FirstPolYear and missing BirthYear (44 rows)\n",
    "df = df[~df.FirstPolYear.isna()]\n",
    "df = df[~df.BirthYear.isna()]\n",
    "\n",
    "# remove rows with missing EducDeg\n",
    "# removes 2 rows (only 2 NaN remaining after removing missing years)\n",
    "df = df[~df.EducDeg.isna()]\n",
    "\n",
    "# replace NaN in Premiums with 0\n",
    "# assumes no info about Premiums means no premium is paid\n",
    "df.PremMotor.fillna(0, inplace = True)\n",
    "df.PremHealth.fillna(0, inplace = True)\n",
    "df.PremLife.fillna(0, inplace = True)\n",
    "df.PremWork.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression model to impute missing MonthSal values\n",
    "# based on high correlation between age and salary\n",
    "\n",
    "# independent, X, and dependent, y, variables\n",
    "X = df.dropna().BirthYear\n",
    "y = df.dropna().MonthSal\n",
    "\n",
    "# split train and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 15)\n",
    "\n",
    "# reshaping needed because of single feature\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "# create and fit model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# predict salary of the validation set\n",
    "y_pred = lin_reg.predict(X_val)\n",
    "\n",
    "# evaluate predictions\n",
    "stats.summary(clf = lin_reg, X = X_train, y = y_train)\n",
    "mse = metrics.mean_squared_error(y_val, y_pred)\n",
    "rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n",
    "mae = metrics.mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(\"\\nMean square error:\", round(mse, 2))\n",
    "print(\"Root mean square error:\", round(rmse, 2))\n",
    "print(\"Mean absolute error:\", round(mae, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict MonthSal NaN values\n",
    "X_test = np.array(df[df.MonthSal.isna()].BirthYear).reshape(-1, 1)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# impute values to MonthSal NaN\n",
    "df.loc[df.MonthSal.isna(), \"MonthSal\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b905273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple linear regression model to impute missing MonthSal values\n",
    "# use all features but the MonthSal to train the model\n",
    "\n",
    "# define independent and dependent variables\n",
    "#X = df.dropna().drop([\"MonthSal\"], axis = 1)\n",
    "#y = df.dropna().MonthSal\n",
    "\n",
    "# split train and test data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)\n",
    "\n",
    "# scale train and test data\n",
    "#scaler = MinMaxScaler().fit(X_train)\n",
    "#X_train_scaled = scaler.transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# create and fit model\n",
    "#lin_model = LinearRegression()\n",
    "#lin_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict y\n",
    "#y_pred = lin_model.predict(X_test_scaled)\n",
    "\n",
    "# evaluate the predictions of the linear reg model\n",
    "#xlabels = X_train.columns\n",
    "#stats.summary(clf = lin_model, X = X_train_scaled, y = y_train, xlabels = xlabels)\n",
    "#mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "#rmse = metrics.mean_squared_error(y_test, y_pred, squared = False)\n",
    "#mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "#print(mse)\n",
    "#print(rmse)\n",
    "#print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression to impute missing Children values\n",
    "\n",
    "# independent, X, and dependent, y, variables\n",
    "X = df.dropna().drop(columns = \"Children\")\n",
    "y = df.dropna().Children\n",
    "\n",
    "# split data into train (70%) and validation (30%) datasets\n",
    "# 70% have children, 30% dont, decided to stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "# scale features using MinMaxScaler() with parameters from X_train\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# scale the training set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# scale the test set\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86551b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features for logistic regression of Children\n",
    "\n",
    "# recursive feature elimination\n",
    "numfeats_list = np.arange(1, len(df.columns))\n",
    "scores = {}\n",
    "\n",
    "for n in range(len(numfeats_list)):\n",
    "    log_reg = LogisticRegression()\n",
    "    rfe = RFE(log_reg, numfeats_list[n])\n",
    "    \n",
    "    X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "    X_val_rfe = rfe.transform(X_val_scaled)\n",
    "    log_reg.fit(X_train_rfe, y_train)\n",
    "    \n",
    "    score = log_reg.score(X_val_rfe, y_val)\n",
    "    scores[n+1] = score\n",
    "\n",
    "best_num_feats = max(scores, key = scores.get)\n",
    "rfe = RFE(estimator = log_reg, n_features_to_select = best_num_feats)\n",
    "X_rfe = rfe.fit_transform(X = X_train_scaled, y = y_train)\n",
    "selected_features = pd.Series(rfe.support_, index = X_train.columns, name = \"RFE\")\n",
    "\n",
    "# compute correlation between Children and other features\n",
    "correlations = pd.Series(df.corr().Children, name = \"Correlation\")\n",
    "\n",
    "# compute Lasso coefficients\n",
    "reg = LassoCV()\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "lasso_coef = pd.Series(reg.coef_, index = X_train.columns, name = \"Lasso\")\n",
    "\n",
    "# concatenate features selected by rfe, correlations, and lasso coefficients\n",
    "selection_df = pd.concat([selected_features, correlations, lasso_coef], axis = 1).drop(\"Children\")\n",
    "\n",
    "# plot correlation and lasso coefficients\n",
    "coef_names = [\"Correlation\", \"Lasso\"]\n",
    "\n",
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, ceil(len(coef_names)), figsize = (22, 10))\n",
    "\n",
    "for ax, coef in zip(axes.flatten(), coef_names):\n",
    "    sns.barplot(data = selection_df,\n",
    "                x = coef,\n",
    "                y = selection_df.index,\n",
    "                hue = \"RFE\",\n",
    "                palette = [\"darkgray\", \"palevioletred\"],\n",
    "                order = selection_df.sort_values(coef).index,\n",
    "                ax = ax)\n",
    "    ax.axvline(x = 0, linestyle = \":\", color = \"darkgray\", label = \"_nolegend_\")\n",
    "    ax.set_xlabel(coef, fontsize = 16)\n",
    "    ax.legend(title = \"RFE\", loc = \"upper right\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ec0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing missing Children values\n",
    "# conclusion from feature selection: use only BirthYear\n",
    "\n",
    "# independent, X, and dependent, y, variables\n",
    "X = df.dropna().BirthYear\n",
    "y = df.dropna().Children\n",
    "\n",
    "# split data into train (70%) and validation (30%) datasets\n",
    "# 70% have children, 30% dont, decided to stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "# reshaping needed because of single feature\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "# scale features using MinMaxScaler() with parameters from X_train\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# scale the training set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# scale the test set\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# create a logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict y\n",
    "y_pred = log_reg.predict(X_val_scaled)\n",
    "\n",
    "# evaluate the predictions of the logistic reg model\n",
    "conf_matrix = metrics.confusion_matrix(y_val, y_pred)\n",
    "accuracy = round(metrics.accuracy_score(y_val, y_pred)*100, 2)\n",
    "precision = round(metrics.precision_score(y_val, y_pred)*100, 2)\n",
    "recall = round(metrics.recall_score(y_val, y_pred)*100, 2)\n",
    "f1 = round(metrics.f1_score(y_val, y_pred)*100, 2)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "print(\"Accuracy:\", accuracy, \"%\")\n",
    "print(\"Precision:\", precision, \"%\")\n",
    "print(\"Recall:\", recall, \"%\")\n",
    "print(\"F1 score:\", f1, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05714f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting children prediction using KNN\n",
    "# overall results were worse than with logistic regression\n",
    "\n",
    "#data = df.dropna().drop([\"Children\", \"GeoLivArea\"], axis = 1)\n",
    "#target = df.dropna().Children\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(data, target, train_size=0.70, stratify = target, random_state=5)\n",
    "\n",
    "#modelKNN = KNeighborsClassifier()\n",
    "#modelKNN.fit(X = X_train, y = y_train)\n",
    "#labels_train = modelKNN.predict(X_train)\n",
    "#labels_val = modelKNN.predict(X_val)\n",
    "\n",
    "#conf_matrix = metrics.confusion_matrix(y_val, labels_val)\n",
    "#accuracy = round(metrics.accuracy_score(y_val, labels_val)*100, 2)\n",
    "#precision = round(metrics.precision_score(y_val, labels_val)*100, 2)\n",
    "#recall = round(metrics.recall_score(y_val, labels_val)*100, 2)\n",
    "#f1 = round(metrics.f1_score(y_val, labels_val)*100, 2)\n",
    "\n",
    "#print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "#print(\"Accuracy:\", accuracy, \"%\")\n",
    "#print(\"Precision:\", precision, \"%\")\n",
    "#print(\"Recall:\", recall, \"%\")\n",
    "#print(\"F1 score:\", f1, \"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict Children NaN values\n",
    "X_test = df.loc[df.Children.isna(), \"BirthYear\"]\n",
    "X_test = np.array(X_test).reshape(-1, 1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# impute values to Children NaN\n",
    "df.loc[df.Children.isna(), \"Children\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all NaN values were dealt with\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c72f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be2fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bfb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999be855",
   "metadata": {},
   "source": [
    "### Data transformation and cross-field validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8838eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where policy was made before birth\n",
    "incoherent_dates_nr = len(df[df.BirthYear > df.FirstPolYear])\n",
    "incoherent_dates_pc = round(len(incoherent_dates)/len(df)*100, 1)\n",
    "print(f\"Number of people with a policy before birth: {incoherent_dates_nr} \"\n",
    "      f\"({incoherent_dates_pc}% of the dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# such high number of inconsistencies suggests systematic error\n",
    "# assumption: in these cases BirthYear and FirstPolYear were introduced in the wrong fields\n",
    "\n",
    "# swap FirstPolYear and BirthYear values when birth is after first policy creation\n",
    "row_filter = df.BirthYear > df.FirstPolYear\n",
    "df.loc[row_filter, [\"FirstPolYear\", \"BirthYear\"]] = df.loc[row_filter, [\"BirthYear\", \"FirstPolYear\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace BirthYear with customer Age (in 2016)\n",
    "# age is more intuitive than birth year\n",
    "curr_year = 2016\n",
    "cust_ages = curr_year - df.BirthYear\n",
    "df.insert(2, \"Age\", cust_ages)\n",
    "\n",
    "# drop BirthYear as it provides the same information\n",
    "df.drop(columns = \"BirthYear\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some stats regarding the Age column\n",
    "df.Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5417e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate EducDeg and Age\n",
    "# minimum age is 18 - everyone can have an education up to High School\n",
    "# minimum age of 20 for a BSc\n",
    "# minimum age of 23 for a PhD (skipping MSc and finishing in 3 years, UK or outside of the EU)\n",
    "print(\"Minimum age associated to each Education Degree:\")\n",
    "educdeg_min_age = df.groupby(\"EducDeg\").Age.min().rename(index = educ_mapper)\n",
    "educdeg_min_age\n",
    "\n",
    "# no incoherences in EducDeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c3b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46d755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcecca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eae63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "084abf92",
   "metadata": {},
   "source": [
    "# **Cleaning up the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8203b5e",
   "metadata": {},
   "source": [
    "## Data transformation (and more cross-field validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Age column (present year considered to be 2016)\n",
    "curr_year = 2016\n",
    "\n",
    "# insert ages as the 3rd feature, after BirthYear\n",
    "df.insert(2, \"Age\", curr_year - df.BirthYear)\n",
    "\n",
    "# plot distribution of ages\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Customer ages\")\n",
    "sns.axes_style(\"dark\")\n",
    "sns.violinplot(y=df[\"Age\"])\n",
    "plt.show()\n",
    "\n",
    "# get some stats regarding the Age column\n",
    "df.Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if EducDeg makes sense according to Age\n",
    "# min age is 18 so everyone can have an education up to High School\n",
    "# check youngest people with a BSc/MSc\n",
    "print(\"Minimum age associated to each Education Degree:\")\n",
    "print(df.groupby(\"EducDeg\").Age.min())\n",
    "\n",
    "# finishing a BSc (EducDeg = 3) at 20 yo is possible if starting at 17\n",
    "# finishing a PhD (EducDeg = 4) at 23 yo is possible if skipping MSc and finishing PhD in 3 years (UK or outside of EU)\n",
    "# no incoherences in EducDeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a FirstPolAge column (present year considered to be 2016)\n",
    "first_pol_age = df.FirstPolYear - df.BirthYear\n",
    "df.insert(1, \"FirstPolAge\", first_pol_age)\n",
    "\n",
    "# plot distribution of age of first policy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Age of first policy\")\n",
    "sns.axes_style(\"dark\")\n",
    "sns.violinplot(y=df[\"FirstPolAge\"])\n",
    "plt.show()\n",
    "\n",
    "# get some stats regarding the Age column\n",
    "df.FirstPolAge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Generation column based on birth year\n",
    "# could use LabelEncoder or OrdinalEncoder here but I couldn't make it work??????????????\n",
    "df.loc[(df.BirthYear >= 1928) & (df.BirthYear <= 1945), \"Generation\"] = 1 # Silent Gen\n",
    "df.loc[(df.BirthYear >= 1946) & (df.BirthYear <= 1964), \"Generation\"] = 2 # Baby Boomer\n",
    "df.loc[(df.BirthYear >= 1965) & (df.BirthYear <= 1980), \"Generation\"] = 3 # Gen X\n",
    "df.loc[(df.BirthYear >= 1981) & (df.BirthYear <= 1995), \"Generation\"] = 4 # Millennial\n",
    "df.loc[(df.BirthYear >= 1996) & (df.BirthYear <= 2010), \"Generation\"] = 5 # Gen Z\n",
    "\n",
    "# generation mapper\n",
    "gen_mapper = {1: \"Silent Gen\",\n",
    "              2: \"Baby Boomer\",\n",
    "              3: \"Gen X\",\n",
    "              4: \"Millennial\",\n",
    "              5: \"Gen Z\"}\n",
    "\n",
    "# convert Generation data to categorical\n",
    "df.Generation = df.Generation.astype(\"category\")\n",
    "\n",
    "# get some stats regarding the Generation column\n",
    "df.Generation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdef9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of customers per generation\n",
    "gen_count = df.groupby(\"Generation\").size().sort_values(ascending = False)\n",
    "gen_count.rename(index = gen_mapper, inplace = True)\n",
    "\n",
    "# plot number of customers per generation\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Number of customers per generation\")\n",
    "sns.axes_style(\"dark\")\n",
    "sns.barplot(x = gen_count.index, y = gen_count.values, order = gen_count.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf040a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a YearSal column\n",
    "# Premiums are also expressed as yearly values\n",
    "# could be interesting to try YearSal as a categorical variable???\n",
    "df.insert(5, \"YearSal\", df.MonthSal*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PremTotal column\n",
    "premium_cols = [\"PremMotor\", \"PremHousehold\", \"PremHealth\", \"PremLife\", \"PremWork\"]\n",
    "df[\"PremTotal\"] = df[premium_cols].sum(axis = 1)\n",
    "\n",
    "# get some stats regarding the PremTotal column\n",
    "print(df.PremTotal.describe())\n",
    "\n",
    "# deal with PremTotal outliers??????????????\n",
    "sns.boxplot(x = df.PremTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no observations where PremTotal is higher than the yearly salary\n",
    "len(df[df.PremTotal > df.YearSal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do when no premiums were paid in 2016?????\n",
    "# it could mean that the insurance was cancelled and they are no longer customers\n",
    "\n",
    "# 12 observations with no Premiums paid in 2016\n",
    "print(f\"{len(df[df.PremTotal == 0])} customer(s) paid no Premium and were removed\")\n",
    "# removed these 12 observations as they likely represent past customers\n",
    "df = df[~(df.PremTotal == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0157171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CustYears column\n",
    "# number of years a customer has been a customer\n",
    "df[\"CustYears\"] = curr_year - df.FirstPolYear\n",
    "\n",
    "# get some stats regarding the CustYears column\n",
    "df.CustYears.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68776bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustMonVal, PremHousehold, PremHealth, PremLife, PremWork may require further processing\n",
    "# histograms\n",
    "skewed_metric_features = [\"CustMonVal\", \"PremHousehold\", \"PremHealth\", \"PremLife\", \"PremWork\", \"PremTotal\"]\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), skewed_metric_features):\n",
    "    sns.histplot(x=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b270c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing the module of the minimum value to all observations\n",
    "\n",
    "# storing pre-transforming premiums for later use\n",
    "no_transform_household = df.PremHousehold.copy()\n",
    "no_transform_health = df.PremHealth.copy()\n",
    "no_transform_life = df.PremLife.copy()\n",
    "no_transform_work = df.PremWork.copy()\n",
    "no_transform_total = df.PremTotal.copy()\n",
    "\n",
    "# applying transformation to normalize distributions\n",
    "df.PremHousehold = np.sqrt(df.PremHousehold + 75)\n",
    "df.PremHealth = np.sqrt(df.PremHealth + 2.11)\n",
    "df.PremLife = np.sqrt(df.PremLife + 7)\n",
    "df.PremWork = np.sqrt(df.PremWork + 12)\n",
    "df.PremTotal = np.sqrt(df.PremTotal)\n",
    "\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize = (20, 11))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), skewed_metric_features):\n",
    "    sns.histplot(x=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of observations discarded\n",
    "num_obs_discarded = len_df-len(df)\n",
    "pc_obs_discarded = round(num_obs_discarded/len_df*100, 2)\n",
    "print(f\"Number of observations discarded: {num_obs_discarded} ({pc_obs_discarded} %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new look at the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52723df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap showing correlation between all metric attributes\n",
    "# pearson, spearman ????????????????\n",
    "plt.subplots(figsize=(15,12))\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "corr_heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "corr_heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high correlations:\n",
    "# FirstPoLYear redundant with CustYears, remove FirstPoLYear as the latter is more intuitive to work with\n",
    "# Age redundant with BirthYear, remove BirthYear as Age is more intuitive to work with\n",
    "# MonthSal redundant with YearSal, remove MonthSal as Premiums are also yearly values\n",
    "# CustMonVal redundant with ClaimsRate, and ClaimsRate not correlated with anything else, remove ClaimsRate\n",
    "# \n",
    "df.drop(columns = [\"FirstPolYear\", \"BirthYear\", \"ClaimsRate\", \"MonthSal\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap showing correlation between the metric attributes selected\n",
    "plt.subplots(figsize=(15,12))\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "corr_heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "corr_heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidimensional representation of metric attributes\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a647d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating metric and non-metric features\n",
    "metric_features = df.select_dtypes(include = np.number).columns.tolist()\n",
    "non_metric_features = df.columns.drop(metric_features).tolist()\n",
    "\n",
    "print(\"Metric Features:\", metric_features)\n",
    "print(\"Non-metric Features:\", non_metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9042fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore EducDeg categorical feature\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(15, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x=df[\"EducDeg\"], y=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ca770",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we do not know much about GeoLivArea or how relevant it might be\n",
    "# explore GeoLivArea categorical feature\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(15, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x=df[\"GeoLivArea\"], y=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a24933",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# explore Children categorical feature\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(15, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x=df[\"Children\"], y=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# kinda weird that median age of people with children is lower than for people without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46d529",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# explore Generation categorical feature\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(15, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x=df[\"Generation\"], y=df[feat], ax=ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children and EducDeg appear to have some discriminating power\n",
    "# Generation also appears to be interesting if we use age as categorical\n",
    "# GeoLivArea does not seem to be particularly useful\n",
    "df.drop(columns = \"GeoLivArea\", inplace = True)\n",
    "\n",
    "# should we retrieve the 1 observation removed because GeoLivArea was NaN??????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganizing dataframe to be more intuitive\n",
    "df = df.loc[:, [\"Generation\",\n",
    "                \"Age\",\n",
    "                \"FirstPolAge\",\n",
    "                \"CustYears\",\n",
    "                \"EducDeg\",\n",
    "                \"Children\",\n",
    "                \"YearSal\",\n",
    "                \"CustMonVal\",\n",
    "                \"PremMotor\",\n",
    "                \"PremHousehold\",\n",
    "                \"PremHealth\",\n",
    "                \"PremLife\",\n",
    "                \"PremWork\",\n",
    "                \"PremTotal\"]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3394a",
   "metadata": {},
   "source": [
    "## Scale metric features and encode categories as binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526db0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating metric and non-metric features\n",
    "\n",
    "# converting to float to have all variables scaled if needed\n",
    "df.EducDeg = df.EducDeg.astype(\"float\")\n",
    "df.Generation = df.Generation.astype(\"float\")\n",
    "df.Children = df.Children.astype(\"float\")\n",
    "\n",
    "metric_features = df.select_dtypes(include = np.number).columns.tolist()\n",
    "non_metric_features = df.columns.drop(metric_features).tolist()\n",
    "\n",
    "print(\"Metric Features:\", metric_features)\n",
    "print(\"Non-metric Features:\", non_metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ce8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale metric features and store them as df_scaled\n",
    "# alternative: StandardScaler (got better results with MinMaxScaler)\n",
    "# create a dataframe with the scaled metric variables\n",
    "df_scaled = df.copy()\n",
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(df_scaled[metric_features])\n",
    "df_scaled[metric_features] = scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are two dataframes at this point\n",
    "# the dataframe obtained prior to normalisation\n",
    "\n",
    "df.PremHousehold = no_transform_household\n",
    "df.PremHealth = no_transform_health\n",
    "df.PremLife = no_transform_life\n",
    "df.PremWork = no_transform_work\n",
    "df.PremTotal = no_transform_total\n",
    "\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe with the scaled metric variables\n",
    "df_scaled.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabde1da",
   "metadata": {},
   "source": [
    "# Sociodemographic clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03ca99",
   "metadata": {},
   "source": [
    "Attempted sociodemographic clustering using different algorithms.\n",
    "KPrototypes was used first to handle Children as a categorical variable, and clusters were agglomerated based on the visualisation of a dendrogram.\n",
    "AgglomerativeClustering and KMeans produce the exact same clusters, and these are very similar to KPrototypes. However, there is perfect split between people with and without children (KPrototypes also splits them into two groups but they are not pure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting sociodemographic clustering\n",
    "# include age, generation, yearly salary, education degree, and children\n",
    "df_sociodem = df[[\"Age\", \"Generation\", \"YearSal\", \"EducDeg\", \"Children\"]]\n",
    "df_sociodem.Children = df_sociodem.Children.astype(\"category\")\n",
    "df_sociodem.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45378463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sociodem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b9c23",
   "metadata": {},
   "source": [
    "## Sociodemographic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba283ff3",
   "metadata": {},
   "source": [
    "For sociodemographic clustering, we used the following algorithms (and features):\n",
    "1. KPrototypes (Age/Generation, EducDeg, Children)\n",
    "2. Agglomerative Clustering (Age/Generation, EducDeg, Children)\n",
    "3. Agglomerative Clustering (Age/Generation, EducDeg)\n",
    "4. KMeans (Age/Generation, EducDeg, Children)\n",
    "5. KMeans ((Age/Generation, EducDeg)\n",
    "\n",
    "Tried DBScan and Mean Shift but because they are density based the weight they give to the Children binary value makes it so that only two clusters are found. Discarding Children and using only Age and EducDeg did not lead to better solutions.\n",
    "\n",
    "Agglomerative Clustering and KMeans, because they are distance-based clustering algorithms, end up giving a lot of weight to binary variables, in this case Children. When analysing the results and the previous box plots, we concluded that while presence or absence of Children had some impact in a couple of the Premiums, that did not justify the weight they were receiving using these algorithms. For that reason, we tried both of them after discarding the feature Children. However, we felt uncomfortable losing Children as the boxplot suggests that it affects how much people pay for their Health insurance, which is the only Premium that does not appear to significantly change with EducDeg and it is not at all correlated to Age.\n",
    "\n",
    "We opted with KMeans (k = 7) as even though it attributes significant weight to children (being a binary variable), the results are similar to KPrototypes (k = 6), with the added advantage of allowing the identification of the sociodemographic group with the highest CMV by far. So overweighing children did not appear to bias the final conclusions. Plus, KPrototypes is excruciantingly slow.\n",
    "\n",
    "In KPrototypes, we tried 3 approaches:\n",
    "1. Create a high number of clusters (15) and agglomerate based on distance (as determined via a dendrogram)\n",
    "2. Create 4 clusters directly\n",
    "3. Create 6 clusters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405156e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this cell if not using hierarchical clustering\n",
    "\n",
    "def hierarchical_clusters(df, n_clusters = 2, threshold = None, affinity = \"euclidean\", linkage = \"ward\"):\n",
    "    \n",
    "    # determine clusters\n",
    "    clusters = AgglomerativeClustering(n_clusters = n_clusters,\n",
    "                                       affinity = affinity,\n",
    "                                       linkage = linkage,\n",
    "                                       distance_threshold = threshold)\n",
    "    clusters.fit(df)\n",
    "    \n",
    "    # retrieve cluster labels and distances\n",
    "    labels = clusters.labels_\n",
    "    distances = clusters.distances_\n",
    "    \n",
    "    counts = np.zeros(clusters.children_.shape[0])\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    for i, merge in enumerate(clusters.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack([clusters.children_, distances, counts]).astype(float)\n",
    "    \n",
    "    return (labels, distances, linkage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c636b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automate categorical feature detection\n",
    "\n",
    "def elbow_plot(df, nmax_clusters):\n",
    "    \n",
    "    n_clusters = np.arange(2, nmax_clusters+1)\n",
    "    cost_vals = []\n",
    "    \n",
    "    for n in n_clusters:\n",
    "        kp_clusters = KPrototypes(n_clusters = n, init = \"Huang\", random_state = 15)\n",
    "        kp_clusters.fit(df, categorical = [2])\n",
    "        cost = kp_clusters.cost_\n",
    "        cost_vals.append(cost)\n",
    "        \n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    sns.lineplot(x = np.arange(2, nmax_clusters + 1), y = cost_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330836d",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KP 1 - Age, EducDeg, Children (k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp1_sociodem = pd.concat([df_scaled[[\"Age\", \"EducDeg\"]], df[\"Children\"]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62257538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow_plot(kp1_sociodem, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72288e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp1_clusters = KPrototypes(n_clusters = 4, init = \"Huang\", random_state = 15)\n",
    "#kp1_clusters.fit(kp1_sociodem, categorical = [2])\n",
    "#kp1_labels = kp1_clusters.labels_\n",
    "#kp1_centroids = kp1_clusters.cluster_centroids_\n",
    "#kp1_linkage = hierarchy.linkage(kp1_centroids, method = \"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3662cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchy.dendrogram(kp1_linkage, color_threshold = 2.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Cluster\"] = kp1_labels\n",
    "#df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2f066",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KP2 - Age, EducDeg, Children (k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp2_clusters = KPrototypes(n_clusters = 6, init = \"Huang\", random_state = 15)\n",
    "#kp2_clusters.fit(kp1_sociodem, categorical = [2])\n",
    "#kp2_labels = kp2_clusters.labels_\n",
    "#kp2_centroids = kp2_clusters.cluster_centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e62e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp2_linkage = hierarchy.linkage(kp2_centroids, method = \"ward\")\n",
    "#hierarchy.dendrogram(kp2_linkage, color_threshold = 2.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb96290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Cluster\"] = kp2_labels\n",
    "#df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc53aea",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KMeans 2 - Age, EducDeg, Children\n",
    "\n",
    "KMeans + Hierarchical Clustering produced very similar results, so might as well go with this (simpler) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a99aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "km1_sociodem = df_scaled[[\"Age\", \"EducDeg\", \"Children\"]]\n",
    "# km2_sociodem = pd.concat([df_scaled[[\"Age\", \"EducDeg\"]], df[\"Children\"]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec89c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax_clusters = 15\n",
    "n_clusters = np.arange(2, nmax_clusters + 1)\n",
    "inertia_vals = []\n",
    "    \n",
    "for n in n_clusters:\n",
    "    km_clusters = KMeans(n_clusters = n, random_state = 15)\n",
    "    km_clusters.fit(km1_sociodem)\n",
    "    inertia = km_clusters.inertia_\n",
    "    inertia_vals.append(inertia)\n",
    "        \n",
    "plt.subplots(figsize=(8, 8))\n",
    "sns.lineplot(x = np.arange(2, nmax_clusters + 1), y = inertia_vals)\n",
    "\n",
    "# elbow plot indicates 6-8 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "km1_clusters = KMeans(n_clusters = 7)\n",
    "km1_clusters.fit(km1_sociodem)\n",
    "km1_labels = km1_clusters.labels_\n",
    "df[\"Cluster\"] = km1_labels\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25965fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
