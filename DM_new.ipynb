{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62cb2cf",
   "metadata": {},
   "source": [
    "# PROJECT -  A2Z CUSTOMER SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060c14e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Data herein presented pertains to a fictional insurance company in Portugal, A2Z Insurance. The goal is to develop a customer segmentation strategy that will enable the Marketing Department to better understand the different customers' profiles and develop adequate marketing strategies. <br>\n",
    "This project is done within the cope of the **Data Mining** curricular unit of the Master's Degree in **Data Science and Advanced Analytics**.\n",
    "\n",
    "#### Group elements:\n",
    "* Ivan Jure ParaÄ‡ (20210689)\n",
    "* Nuno de Bourbon e Carvalho Melo (20210681)\n",
    "* Stuart Gallina Ottersen (20210703)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data exploration](#data-exploration)\n",
    "2. [Data preprocessing](#data-preprocessing)\n",
    "    1. [First steps](#preprocessing-first-steps)\n",
    "    2. [Dealing with outliers](#preprocessing-outliers)\n",
    "    3. [Handling missing values](#preprocessing-missing-values)\n",
    "    4. [Data transformation and cross field validation](#preprocessing-transform-validate)\n",
    "    5. [Feature selection](#preprocessing-feature-selection)\n",
    "    6. [Feature skewness and scaling](#preprocessing-scaling)\n",
    "3. [Clustering](#data-clustering)\n",
    "    1. [Sociodemographic clustering](#clustering-sociodemographic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832a0d1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2><center>BOILERPLATE</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d408b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line of code to install package required for KPrototypes\n",
    "# !pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be438b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import major libraries/modules\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# others\n",
    "from math import ceil\n",
    "from regressors import stats\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import chi2_contingency\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, AffinityPropagation, OPTICS, MeanShift\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4952e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de61fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SAS file with the insurance company data\n",
    "df, meta = pyreadstat.read_sas7bdat('a2z_insurance.sas7bdat')\n",
    "\n",
    "# save copy of the original dataframe\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31d92",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-exploration\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA EXPLORATION</center></h2>\n",
    "\n",
    "***\n",
    "\n",
    "* First look at the dataset\n",
    "* Set customer ID number as index\n",
    "* Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17472222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows and columns\n",
    "print(\"Number of observations:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])\n",
    "print(\"Features:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "df.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7724d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-preprocessing\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>DATA PREPROCESSING</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3b73b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-first-steps\"></a>\n",
    "\n",
    "<h3><right>First steps</right></h3>\n",
    "\n",
    "* Set customer ID as index\n",
    "* Remove duplicated observations\n",
    "* Convert EducDeg to float\n",
    "* Swap incoherent birth and first policy years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set customer ID as index\n",
    "df.CustID = df.CustID.astype(\"int\")\n",
    "df.set_index(\"CustID\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicated rows\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# remove duplicate rows\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(\"Removing duplicates...\")\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store initial number of rows (after duplicate removal)\n",
    "initial_len = len(df)\n",
    "\n",
    "# check number of rows and columns again\n",
    "print(\"Number of observations:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data types\n",
    "\n",
    "# extract number from EducDeg, save as float\n",
    "df.EducDeg = df.EducDeg.str.extract(\"(\\d+)\").astype(\"float\")\n",
    "# education degree mapper (to replace numbers when needed)\n",
    "educ_mapper = {1: \"Basic\", 2: \"High School\", 3: \"BSc/MSc\", 4: \"PhD\"}\n",
    "\n",
    "# check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swaps years if policy seems to be made before birth\n",
    "nr_incoherences = len(df[df.BirthYear > df.FirstPolYear])\n",
    "pc_incoherences = round(nr_incoherences/len(df)*100, 1)\n",
    "\n",
    "print(f\"Number of people with a policy before birth: {nr_incoherences} \"\n",
    "    f\"({pc_incoherences}% of the dataset)\")\n",
    "\n",
    "# such high number of inconsistencies suggests systematic error\n",
    "# assumption: in these cases BirthYear and FirstPolYear were introduced in the wrong fields\n",
    "print(\"Swapping birth and first policy years...\")\n",
    "\n",
    "# swap FirstPolYear and BirthYear values when birth occurs after first policy creation\n",
    "row_filter = df.BirthYear > df.FirstPolYear\n",
    "df.loc[row_filter, [\"FirstPolYear\", \"BirthYear\"]] = df.loc[row_filter, [\"BirthYear\", \"FirstPolYear\"]].values\n",
    "\n",
    "# check if the years were correctly swapped\n",
    "nr_incoherences = len(df[df.BirthYear > df.FirstPolYear])\n",
    "print(f\"Number of people with a policy before birth: {nr_incoherences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7f8fd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-outliers\"></a>\n",
    "\n",
    "<h3><right>Dealing with outliers</right></h3>\n",
    "\n",
    "* Visually inspect and remove outliers\n",
    "* Store outliers in a separate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplots(df, features):\n",
    "\n",
    "    sns.set()\n",
    "\n",
    "    fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize = (20, 11))\n",
    "    \n",
    "    # iterate through axes objects and associate each box plot\n",
    "    for ax, feat in zip(axes.flatten(), metric_features):\n",
    "        sns.boxplot(x = df[feat], ax = ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "\n",
    "    # BirthYear 1028 assumed to be a typo\n",
    "    # 0 and 9 are fairly close in a qwerty keyboard, replaced with 1928\n",
    "    df.loc[df.BirthYear == 1028, \"BirthYear\"] = 1928\n",
    "\n",
    "    # conditions to remove outliers\n",
    "    filters = ((df.FirstPolYear.ge(2017)),\n",
    "            (df.MonthSal.ge(20000)),\n",
    "            (df.CustMonVal.le(-2000)),\n",
    "            (df.CustMonVal.ge(1500)),\n",
    "            (df.ClaimsRate.ge(4)),\n",
    "            (df.PremMotor.ge(3000)),\n",
    "            (df.PremHousehold.ge(1600)),\n",
    "            (df.PremHealth.ge(420)),\n",
    "            (df.PremLife.ge(290)),\n",
    "            (df.PremWork.ge(300)))\n",
    "\n",
    "    # create a separate dataframe for the outliers\n",
    "    df_outliers = pd.DataFrame()\n",
    "\n",
    "    # remove outliers from main dataframe\n",
    "    # add outliers to df_outliers\n",
    "    for filter_ in filters:\n",
    "        df_outliers = df_outliers.append(df[filter_])\n",
    "        df = df[~filter_]\n",
    "        \n",
    "    # determine number of outliers removed\n",
    "    n_outliers = len(df_outliers)\n",
    "    pc_removed = round(n_outliers/initial_len*100, 2)\n",
    "    print(f\"Number of outliers removed: {n_outliers} ({pc_removed}% of all observations)\")\n",
    "\n",
    "    return (df, df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03192d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\"]\n",
    "\n",
    "# boxplots of metric features\n",
    "get_boxplots(df, metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from df, store them in df_outliers\n",
    "df, df_outliers = remove_outliers(df)\n",
    "\n",
    "# boxplots of metric features after removing outliers\n",
    "get_boxplots(df, metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new look at the data after removing outliers\n",
    "df.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625bc27",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-missing-values\"></a>\n",
    "\n",
    "<h3><right>Handling missing values</right></h3>\n",
    "\n",
    "* Check feature and row completeness (in df and df_outliers)\n",
    "* Remove customers with no information about Premiums\n",
    "* Remove customers with missing FirstPolYear or BirthYear\n",
    "* Remove customers with missing EducDeg\n",
    "* Fill Premium missing values with zero\n",
    "* Create Linear Regression model to impute MonthSal\n",
    "* Create Logistic Regression model to impute Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(df):\n",
    "\n",
    "    # check feature completeness\n",
    "    # number and percentage of NaN values per feature\n",
    "    nr_nans = df.isna().sum()\n",
    "    pc_nans = df.isna().mean()*100\n",
    "    feature_nans = pd.concat([nr_nans, pc_nans], axis = 1)\n",
    "    feature_nans.rename(columns = {0: \"nr\", 1: \"%\"}, inplace = True)\n",
    "\n",
    "    # show number of missing values per feature\n",
    "    print(\"Missing values per feature:\\n\", feature_nans)\n",
    "\n",
    "    # check row completeness\n",
    "    # max number of NaN values in a row and number of rows with that many NaN\n",
    "    max_row_nan = df.isnull().sum(axis = 1).max()\n",
    "    \n",
    "    print(f\"\\nMaximum number of NaN values per row: {max_row_nan} \"\n",
    "        f\"({len(df[df.isnull().sum(axis = 1) == max_row_nan])} rows)\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd056b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(df):\n",
    "\n",
    "    # create dataframe to store rows with missing values discarded from the main dataframe\n",
    "    df_nan = pd.DataFrame()\n",
    "\n",
    "    # a row with 4 NaN (max number of missing values) has ~30% of its data missing (4 out of 13 features)\n",
    "    # removing these rows - no information about Premiums\n",
    "    max_nan_rows = df[df.isnull().sum(axis = 1) == df.isnull().sum(axis = 1).max()]\n",
    "    df_nan = df_nan.append(max_nan_rows)\n",
    "    df.drop(max_nan_rows.index, inplace = True)\n",
    "    \n",
    "    # remove rows with missing FirstPolYear and missing BirthYear\n",
    "    firstpolyear_nan = df.FirstPolYear.isna()\n",
    "    birthyear_nan = df.BirthYear.isna()\n",
    "    df_nan = df_nan.append(df[firstpolyear_nan])\n",
    "    df_nan = df_nan.append(df[birthyear_nan])\n",
    "    df = df[~firstpolyear_nan]\n",
    "    df = df[~birthyear_nan]\n",
    "\n",
    "    # remove rows with missing EducDeg\n",
    "    educ_nan = df.EducDeg.isna()\n",
    "    df_nan = df_nan.append(df[educ_nan])\n",
    "    df = df[~educ_nan]\n",
    "\n",
    "    # make the function more verbose\n",
    "    # show number of missing values removed\n",
    "    nr_rows_removed = len(df_nan)\n",
    "    total_removed = initial_len - len(df)\n",
    "\n",
    "    print(f\"Removed {len(max_nan_rows)} rows (~30% missing data, no information about Premiums)\")\n",
    "    print(f\"Removed {sum(firstpolyear_nan)} rows (missing values in FirstPolYear)\")\n",
    "    print(f\"Removed {sum(birthyear_nan)} rows (missing values in BirthYear)\")\n",
    "    print(f\"Removed {sum(educ_nan)} rows (missing values in EducDeg)\")\n",
    "    print(f\"Total number of rows removed: {nr_rows_removed}\")\n",
    "    print(f\"Total number of rows removed so far: {total_removed} ({round(total_removed/initial_len*100, 2)}%)\")\n",
    "\n",
    "    return (df, df_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_premiums(df):\n",
    "\n",
    "    # replace NaN in Premiums with 0\n",
    "    # assume no info about Premiums means no premium is paid\n",
    "    motor_nan = df.PremMotor.isna()\n",
    "    health_nan = df.PremHealth.isna()\n",
    "    life_nan = df.PremLife.isna()\n",
    "    work_nan = df.PremWork.isna()\n",
    "    total_nan = motor_nan.sum() + health_nan.sum() + life_nan.sum() + work_nan.sum()\n",
    "\n",
    "    df.PremMotor.fillna(0, inplace = True)\n",
    "    df.PremHealth.fillna(0, inplace = True)\n",
    "    df.PremLife.fillna(0, inplace = True)\n",
    "    df.PremWork.fillna(0, inplace = True)\n",
    "\n",
    "    # make the function verbose\n",
    "    # show number of values imputed\n",
    "    print(f\"Imputed {sum(motor_nan)} values in PremMotor\")\n",
    "    print(f\"Imputed {sum(health_nan)} values in PremHealth\")\n",
    "    print(f\"Imputed {sum(life_nan)} values in PremLife\")\n",
    "    print(f\"Imputed {sum(work_nan)} values in PremWork\")\n",
    "    print(f\"Total number of imputations: {total_nan}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_salaries(df, regressors):\n",
    "\n",
    "    # independent, X, and dependent, y, variables\n",
    "    X = df.dropna()[regressors]\n",
    "    y = df.dropna().MonthSal\n",
    "\n",
    "    # split train and validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 15)\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_train = np.array(X_train).reshape(-1, 1)\n",
    "        X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "    # scale features using MinMaxScaler() with parameters from X_train\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    # scale the training and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # create and fit model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict salary of the validation set\n",
    "    y_pred = lin_reg.predict(X_val_scaled)\n",
    "\n",
    "    # compute metrics for the predictions made\n",
    "    mse = metrics.mean_squared_error(y_val, y_pred)\n",
    "    rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n",
    "    mae = metrics.mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"================================================\")\n",
    "    print(\"        MonthSal linear regression model        \")\n",
    "    print(\"================================================\")\n",
    "    print(\"Mean square error:\", round(mse, 2))\n",
    "    print(\"Root mean square error:\", round(rmse, 2))\n",
    "    print(\"Mean absolute error:\", round(mae, 2))\n",
    "    stats.summary(clf = lin_reg, X = X_train_scaled, y = y_train)\n",
    "\n",
    "    # predict MonthSal NaN values\n",
    "    X_test = df.loc[df.MonthSal.isna(), regressors]\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_test = np.array(X_test).reshape(-1, 1)\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = lin_reg.predict(X_test_scaled)\n",
    "    print(y_pred)\n",
    "\n",
    "    # impute values to MonthSal NaN\n",
    "    print(f\"\\nImputed {len(y_pred)} values in MonthSal\")\n",
    "    #df.loc[df.MonthSal.isna(), \"MonthSal\"] = y_pred\n",
    "\n",
    "    # multiple linear regression model to impute missing MonthSal values\n",
    "    # use all features but the MonthSal to train the model\n",
    "\n",
    "    # define independent and dependent variables\n",
    "    #X = df.dropna().drop([\"MonthSal\"], axis = 1)\n",
    "    #y = df.dropna().MonthSal\n",
    "\n",
    "    # split train and test data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)\n",
    "\n",
    "    # scale train and test data\n",
    "    #scaler = MinMaxScaler().fit(X_train)\n",
    "    #X_train_scaled = scaler.transform(X_train)\n",
    "    #X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # create and fit model\n",
    "    #lin_model = LinearRegression()\n",
    "    #lin_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict y\n",
    "    #y_pred = lin_model.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the predictions of the linear reg model\n",
    "    #xlabels = X_train.columns\n",
    "    #stats.summary(clf = lin_model, X = X_train_scaled, y = y_train, xlabels = xlabels)\n",
    "    #mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    #rmse = metrics.mean_squared_error(y_test, y_pred, squared = False)\n",
    "    #mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    #print(mse)\n",
    "    #print(rmse)\n",
    "    #print(mae)\n",
    "\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_children(df, regressors):\n",
    "            \n",
    "    # imputing missing Children values\n",
    "    # conclusion based on previous feature selection: use only BirthYear\n",
    "\n",
    "    # independent, X, and dependent, y, variables\n",
    "    X = df.dropna()[regressors]\n",
    "    y = df.dropna().Children\n",
    "\n",
    "    # split data into train (70%) and validation (30%) datasets\n",
    "    # 70% have children, 30% dont, decided to stratify\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_train = np.array(X_train).reshape(-1, 1)\n",
    "        X_val = np.array(X_val).reshape(-1, 1)\n",
    "\n",
    "    # scale features using MinMaxScaler() with parameters from X_train\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    # scale the training and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # create a logistic regression model\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict y\n",
    "    y_pred = log_reg.predict(X_val_scaled)\n",
    "\n",
    "    # evaluate the predictions of the logistic reg model\n",
    "    conf_matrix = metrics.confusion_matrix(y_val, y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix)\n",
    "    accuracy = round(metrics.accuracy_score(y_val, y_pred)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_val, y_pred)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_val, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_val, y_pred)*100, 2)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"   Children logistic regression model   \")\n",
    "    print(\"========================================\")\n",
    "    print(\"Accuracy:\", accuracy, \"%\")\n",
    "    print(\"Precision:\", precision, \"%\")\n",
    "    print(\"Recall:\", recall, \"%\")\n",
    "    print(\"F1 score:\", f1, \"%\")\n",
    "    print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "\n",
    "    # predict Children NaN values\n",
    "    X_test = df.loc[df.Children.isna(), regressors]\n",
    "\n",
    "    # reshaping needed if a string and not a list is passed as argument for regressors\n",
    "    if type(regressors) is not list:\n",
    "        X_test = np.array(X_test).reshape(-1, 1)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "    # impute values to Children NaN\n",
    "    df.loc[df.Children.isna(), \"Children\"] = y_pred\n",
    "\n",
    "    # attempting children prediction using KNN\n",
    "    # overall results were worse than with logistic regression\n",
    "\n",
    "    #data = df.dropna().drop([\"Children\", \"GeoLivArea\"], axis = 1)\n",
    "    #target = df.dropna().Children\n",
    "\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(data, target, train_size=0.70, stratify = target, random_state=5)\n",
    "\n",
    "    #modelKNN = KNeighborsClassifier()\n",
    "    #modelKNN.fit(X = X_train, y = y_train)\n",
    "    #labels_train = modelKNN.predict(X_train)\n",
    "    #labels_val = modelKNN.predict(X_val)\n",
    "\n",
    "    #conf_matrix = metrics.confusion_matrix(y_val, labels_val)\n",
    "    #accuracy = round(metrics.accuracy_score(y_val, labels_val)*100, 2)\n",
    "    #precision = round(metrics.precision_score(y_val, labels_val)*100, 2)\n",
    "    #recall = round(metrics.recall_score(y_val, labels_val)*100, 2)\n",
    "    #f1 = round(metrics.f1_score(y_val, labels_val)*100, 2)\n",
    "\n",
    "    #print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "    #print(\"Accuracy:\", accuracy, \"%\")\n",
    "    #print(\"Precision:\", precision, \"%\")\n",
    "    #print(\"Recall:\", recall, \"%\")\n",
    "    #print(\"F1 score:\", f1, \"%\") \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in the dataframe\n",
    "check_completeness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops missing values\n",
    "df, df_nan = remove_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45537541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in the Premium features\n",
    "df = impute_premiums(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature(s) to use for linear regression of MonthSal\n",
    "corr_salary = pd.Series(df.corr().MonthSal, name = \"Correlation\").sort_values()\n",
    "print(\"Correlation between salary and other features:\")\n",
    "print(round(corr_salary, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing MonthSal values based on linear regression\n",
    "salary_regressors = \"BirthYear\"\n",
    "df = impute_salaries(df, salary_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers dataframe for NaN values\n",
    "outliers_nan_before = df_outliers.isna().sum()\n",
    "\n",
    "# only 3 NaN - 1 PremMotor, 1 PremHealth, 1 PremLife\n",
    "# assuming no info about premiums means no premium is paid\n",
    "df_outliers.PremMotor.fillna(0, inplace = True)\n",
    "df_outliers.PremHealth.fillna(0, inplace = True)\n",
    "df_outliers.PremLife.fillna(0, inplace = True)\n",
    "\n",
    "# check if NaN values were correctly imputed\n",
    "outliers_nan_after = df_outliers.isna().sum()\n",
    "outliers_nan = pd.concat([outliers_nan_before, outliers_nan_after], axis = 1)\n",
    "outliers_nan.rename(columns = {0: \"before\", 1: \"after\"}, inplace = True)\n",
    "\n",
    "print(\"Missing values in the outliers' dataframe:\")\n",
    "outliers_nan.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86551b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features for logistic regression of Children\n",
    "\n",
    "# independent, X, and dependent, y, variables\n",
    "X = df.dropna().drop(columns = \"Children\")\n",
    "y = df.dropna().Children\n",
    "\n",
    "# split data into train (70%) and validation (30%) datasets\n",
    "# 70% have children, 30% dont, decided to stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 5, stratify = y)\n",
    "\n",
    "# scale features using MinMaxScaler() with parameters from X_train\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# scale the training set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# scale the test set\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# recursive feature elimination\n",
    "numfeats_list = np.arange(1, len(df.columns))\n",
    "scores = {}\n",
    "\n",
    "for n in range(len(numfeats_list)):\n",
    "    log_reg = LogisticRegression()\n",
    "    rfe = RFE(log_reg, numfeats_list[n])\n",
    "    \n",
    "    X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "    X_val_rfe = rfe.transform(X_val_scaled)\n",
    "    log_reg.fit(X_train_rfe, y_train)\n",
    "    \n",
    "    score = log_reg.score(X_val_rfe, y_val)\n",
    "    scores[n+1] = score\n",
    "\n",
    "# RFE selected 1 single feature\n",
    "best_num_feats = max(scores, key = scores.get)\n",
    "rfe = RFE(estimator = log_reg, n_features_to_select = best_num_feats)\n",
    "X_rfe = rfe.fit_transform(X = X_train_scaled, y = y_train)\n",
    "selected_features = pd.Series(rfe.support_, index = X_train.columns, name = \"RFE\")\n",
    "\n",
    "# compute correlation between Children and other features\n",
    "corr_children = pd.Series(df.corr().Children, name = \"Correlation\")\n",
    "\n",
    "# compute Lasso coefficients\n",
    "reg = LassoCV()\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "lasso_coef = pd.Series(reg.coef_, index = X_train.columns, name = \"Lasso\")\n",
    "\n",
    "# concatenate features selected by rfe, correlations, and lasso coefficients\n",
    "selection_df = pd.concat([selected_features, corr_children, lasso_coef], axis = 1).drop(\"Children\")\n",
    "\n",
    "# plot correlation and lasso coefficients\n",
    "coef_names = [\"Correlation\", \"Lasso\"]\n",
    "\n",
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, ceil(len(coef_names)), figsize = (24, 10))\n",
    "\n",
    "for ax, coef in zip(axes.flatten(), coef_names):\n",
    "    sns.barplot(data = selection_df,\n",
    "                x = coef,\n",
    "                y = selection_df.index,\n",
    "                hue = \"RFE\",\n",
    "                palette = [\"darkgray\", \"C9\"],\n",
    "                order = selection_df.sort_values(coef).index,\n",
    "                ax = ax)\n",
    "    ax.axvline(x = 0, linestyle = \"--\", color = \"darkgray\", label = \"_nolegend_\")\n",
    "    ax.set_xlabel(coef + \" coefficient\", fontsize = 16)\n",
    "    ax.legend(title = \"RFE\", loc = \"upper right\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daef62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute children missing values based on selected features\n",
    "children_regressors = [\"BirthYear\"]\n",
    "df = impute_children(df, children_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all NaN values were dealt with\n",
    "check_completeness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b7805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999be855",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-transform-validate\"></a>\n",
    "\n",
    "<h3><right>Data transformation and cross field validation</right></h3>\n",
    "\n",
    "* Add columns: Age, FirstPolAge, CustYears, Generation, PremTotal, Premium ratios\n",
    "* Check if EducDeg makes sense given the customer's age\n",
    "* Convert MonthSal to YearSal\n",
    "* Remove outliers from newly created features\n",
    "* Transform skewed metric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creator(df):\n",
    "    \n",
    "    # create Age feature\n",
    "    curr_year = 2016\n",
    "    cust_ages = curr_year - df.BirthYear\n",
    "    df[\"Age\"] = cust_ages\n",
    "    \n",
    "    # create FirstPolAge feature\n",
    "    firstpol_ages = df.FirstPolYear - df.BirthYear\n",
    "    df[\"FirstPolAge\"] = firstpol_ages\n",
    "    \n",
    "    # create CustYears feature\n",
    "    df[\"CustYears\"] = curr_year - df.FirstPolYear\n",
    "    \n",
    "    # create a Generation feature\n",
    "    df.loc[(df.BirthYear >= 1928) & (df.BirthYear <= 1945), \"Generation\"] = 1 # Silent Gen\n",
    "    df.loc[(df.BirthYear >= 1946) & (df.BirthYear <= 1964), \"Generation\"] = 2 # Baby Boomer\n",
    "    df.loc[(df.BirthYear >= 1965) & (df.BirthYear <= 1980), \"Generation\"] = 3 # Gen X\n",
    "    df.loc[(df.BirthYear >= 1981) & (df.BirthYear <= 1996), \"Generation\"] = 4 # Millennial\n",
    "    df.loc[(df.BirthYear >= 1997) & (df.BirthYear <= 2012), \"Generation\"] = 5 # Gen Z\n",
    "    df.Generation = df.Generation.astype(\"float\")\n",
    "    \n",
    "    # create a YearSal feature\n",
    "    year_sals = df.MonthSal * 12\n",
    "    df[\"YearSal\"] = year_sals\n",
    "    \n",
    "    # create a PremTotal column\n",
    "    premium_cols = [\"PremMotor\", \"PremHousehold\", \"PremHealth\", \"PremLife\", \"PremWork\"]\n",
    "    df[\"PremTotal\"] = df[premium_cols].sum(axis = 1)\n",
    "    \n",
    "    # create PremiumRatio columns\n",
    "    for col in premium_cols:\n",
    "        newcol_vals = df[col]/df[\"PremTotal\"]\n",
    "        newcol_name = col + \"_ratio\"\n",
    "        df[newcol_name] = newcol_vals\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataframe with new features\n",
    "df = feature_creator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5582f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more information about the newly created Age, FirstPolAge, and CustYears features\n",
    "year_newfeatures = [\"Age\", \"FirstPolAge\", \"CustYears\"]\n",
    "\n",
    "# dataframe to store summary statistics\n",
    "feats_info = pd.DataFrame()\n",
    "\n",
    "# violin plots of these three features\n",
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, axes = plt.subplots(1, ceil(len(year_newfeatures)), figsize = (16, 4))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), year_newfeatures):\n",
    "    sns.violinplot(data = df,\n",
    "                   x = df[feat],\n",
    "                   color = \"C9\",\n",
    "                   ax = ax)\n",
    "    ax.set_xlim([-10, 110])\n",
    "    \n",
    "    # append summary statistics to feats_info dataframe\n",
    "    feat_info = pd.Series(df[feat].describe())\n",
    "    feats_info = feats_info.append(feat_info)\n",
    "\n",
    "print(\"=============================================================================================================\")\n",
    "print(\"   Customer age (Age), age at first policy (FirstPolAge), and number of years with the company (CustYears)   \")\n",
    "print(\"=============================================================================================================\")\n",
    "print(feats_info.T)\n",
    "\n",
    "# almost 50% of the customers made their first policy before turning 18\n",
    "pre_18_policy = len(df[df.FirstPolAge < 18])\n",
    "print(\"\\nMade first policy before 18:\", round(pre_18_policy/len(df), 2)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate EducDeg and Age\n",
    "# minimum age is 18 - meaning everyone can have an education up to High School\n",
    "# minimum age of 20 for a BSc\n",
    "# minimum age of 23 for a PhD (skipping MSc and finishing in 3 years, UK or outside of the EU)\n",
    "educdeg_min_age = df.groupby(\"EducDeg\").Age.min().rename(index = educ_mapper)\n",
    "print(\"======================================\")\n",
    "print(\"   Cross-validating EducDeg and Age   \")\n",
    "print(\"======================================\")\n",
    "print(\"Minimum age associated to each EducDeg\")\n",
    "print(educdeg_min_age)\n",
    "\n",
    "# no incoherences found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Generation feature\n",
    "# generation mapper\n",
    "gen_mapper = {1: \"Silent Gen\",\n",
    "              2: \"Baby Boomer\",\n",
    "              3: \"Gen X\",\n",
    "              4: \"Millennial\",\n",
    "              5: \"Gen Z\"}\n",
    "\n",
    "# count number of customers per generation\n",
    "gen_count = df.groupby(\"Generation\").size().sort_values(ascending = False)\n",
    "gen_count.rename(index = gen_mapper, inplace = True)\n",
    "\n",
    "# visualize number of customers per generation\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "sns.barplot(x = gen_count.index, y = gen_count.values, order = gen_count.index)\n",
    "plt.title(\"Number of customers per generation\", fontsize = 22)\n",
    "plt.xlabel(\"Generation\", fontsize = 18)\n",
    "plt.ylabel(\"Count\", fontsize = 18)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# almost no customers from the younger generation, Gen Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"]\n",
    "\n",
    "# boxplot of current metric features\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(5, ceil(len(metric_features) / 5), figsize = (20, 25))\n",
    "\n",
    "# iterate through axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers in newly created features?\n",
    "# conditions to remove outliers\n",
    "filters = ((df.PremHousehold_ratio.ge(0.85)),\n",
    "           (df.PremHealth_ratio.ge(0.7)),\n",
    "           (df.PremLife_ratio.ge(0.5)),\n",
    "           (df.PremWork_ratio.ge(0.4)),\n",
    "           (df.PremWork_ratio.le(-0.05)),\n",
    "           (df.PremTotal.le(350)),\n",
    "           (df.PremTotal.ge(1750)))\n",
    "\n",
    "# number of observations before second round of outliers removal\n",
    "nr_initial_outliers = len(df_outliers)\n",
    "\n",
    "# remove outliers from main dataframe, store them in df_outliers\n",
    "for filter_ in filters:\n",
    "    df_outliers = df_outliers.append(df[filter_])\n",
    "    df = df[~filter_]\n",
    "\n",
    "nr_new_outliers = len(df_outliers) - nr_initial_outliers\n",
    "total_removed = initial_len - len(df)\n",
    "print(f\"Number of outliers removed (2nd round): {nr_new_outliers}\")\n",
    "print(f\"Total observations removed: {total_removed} ({round(total_removed/initial_len*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32249b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of metric features after removing outliers\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(5, ceil(len(metric_features) / 5), figsize = (20, 25))\n",
    "\n",
    "# iterate through axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880bbd4",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-feature-selection\"></a>\n",
    "\n",
    "<h3><right>Feature selection</right></h3>\n",
    "\n",
    "* Remove redundant features\n",
    "* Remove irrelevant features\n",
    "* Prepare dataframe for cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap showing correlation between all features\n",
    "plt.subplots(figsize = (15, 12))\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype = bool))\n",
    "corr_heatmap = sns.heatmap(df.corr(), mask = mask, vmin = -1, vmax = 1, annot = True, cmap = 'BrBG')\n",
    "corr_heatmap.set_title('Triangle Correlation Heatmap', fontdict = {'fontsize': 14}, pad = 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant features from main dataframe (perfect correlation)\n",
    "df.drop(columns = [\"FirstPolYear\", \"BirthYear\", \"MonthSal\"], inplace = True)\n",
    "\n",
    "# TO BE DECIDED (based on future clustering)\n",
    "# Age vs FirstPolAge vs Generation vs YearSal\n",
    "# ClaimsRate vs CustMonVal\n",
    "# PremTotal vs PremHousehold\n",
    "# PremTotal vs PremHousehold_ratio\n",
    "# Prem ratios vs Prem values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap showing correlation between updated features\n",
    "plt.subplots(figsize = (15, 12))\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype = bool))\n",
    "corr_heatmap = sns.heatmap(df.corr(), mask = mask, vmin = -1, vmax = 1, annot = True, cmap = 'BrBG')\n",
    "corr_heatmap.set_title('Triangle Correlation Heatmap', fontdict = {'fontsize': 14}, pad = 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\", \"Children\", \"Generation\"]\n",
    "\n",
    "print(\"Metric features:\", metric_features)\n",
    "print(\"Non-metric features:\", non_metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoLivArea has no meaningful correlation with any of the other features\n",
    "# explore non-metric feature - GeoLivArea\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(6, 3, figsize=(20, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(x = df[\"GeoLivArea\"], y = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# GeoLivArea appears to have no discriminative power\n",
    "# this was already hinted at in the previous correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove GeoLivArea from the main dataframe\n",
    "df.drop(columns = \"GeoLivArea\", inplace = True)\n",
    "\n",
    "# remove GeoLivArea from the outliers dataframe\n",
    "df_outliers.drop(columns = \"GeoLivArea\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee58168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more outliers after creating features resulted in missing values for previous observations\n",
    "nan_outliers_before = pd.Series(df_outliers.isna().sum(), name = \"before\")\n",
    "\n",
    "df_outliers = feature_creator(df_outliers)\n",
    "\n",
    "# check if missing values were dealt with\n",
    "nan_outliers_after = pd.Series(df_outliers.isna().sum(), name = \"after\")\n",
    "nan_outliers = pd.concat([nan_outliers_before, nan_outliers_after], axis = 1).T\n",
    "print(\"Missing values in the outliers dataframe:\")\n",
    "display(nan_outliers)\n",
    "\n",
    "# remove redundant features from outliers dataframe (based on corr heatmap)\n",
    "df_outliers.drop(columns = [\"FirstPolYear\", \"BirthYear\", \"MonthSal\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e72e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_df(df):\n",
    "    \n",
    "    # reorganise column order\n",
    "    df = df.loc[:, [\"Generation\",\n",
    "                   \"Age\",\n",
    "                   \"YearSal\",\n",
    "                   \"EducDeg\",\n",
    "                   \"Children\",\n",
    "                   \"FirstPolAge\",\n",
    "                   \"CustYears\",\n",
    "                   \"CustMonVal\",\n",
    "                   \"ClaimsRate\",\n",
    "                   \"PremMotor\",\n",
    "                   \"PremMotor_ratio\",\n",
    "                   \"PremHousehold\",\n",
    "                   \"PremHousehold_ratio\",\n",
    "                   \"PremHealth\",\n",
    "                   \"PremHealth_ratio\",\n",
    "                   \"PremLife\",\n",
    "                   \"PremLife_ratio\",\n",
    "                   \"PremWork\",\n",
    "                   \"PremWork_ratio\",\n",
    "                   \"PremTotal\"]]\n",
    "    \n",
    "    # rename a few columns\n",
    "    df = df.rename(columns = {\"CustMonVal\": \"CMV\",\n",
    "                             \"PremMotor\": \"Motor\",\n",
    "                             \"PremMotor_ratio\": \"MotorRatio\",\n",
    "                             \"PremHousehold\": \"House\",\n",
    "                             \"PremHousehold_ratio\": \"HouseRatio\",\n",
    "                             \"PremHealth\": \"Health\",\n",
    "                             \"PremHealth_ratio\": \"HealthRatio\",\n",
    "                             \"PremLife\": \"Life\",\n",
    "                             \"PremLife_ratio\": \"LifeRatio\",\n",
    "                             \"PremWork\": \"Work\",\n",
    "                             \"PremWork_ratio\": \"WorkRatio\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# restructure and visualize the outliers dataframe before cluster analysis\n",
    "df_outliers = restructure_df(df_outliers)\n",
    "df_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure and visualize the main dataframe before cluster analysis\n",
    "df = restructure_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metric and non-metric features\n",
    "metric_features = df.columns.drop([\"EducDeg\", \"Children\", \"Generation\"])\n",
    "non_metric_features = [\"EducDeg\", \"Children\", \"Generation\"]\n",
    "\n",
    "print(f\"Final number of observations: {df.shape[0]} ({round(df.shape[0]/initial_len*100, 2)}% of the original dataset)\")\n",
    "print(f\"Current number of features: {df.shape[1]}\")\n",
    "print(\"Features:\")\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3394a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preprocessing-scaling\"></a>\n",
    "\n",
    "<h3><right>Feature skewness and scaling</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with skewed metric features\n",
    "skewed_metric_feats = [\"CMV\",\n",
    "                       \"House\",\n",
    "                       \"HouseRatio\",\n",
    "                       \"Health\",\n",
    "                       \"HealthRatio\",\n",
    "                       \"Life\",\n",
    "                       \"LifeRatio\",\n",
    "                       \"Work\",\n",
    "                       \"WorkRatio\",\n",
    "                       \"PremTotal\"]\n",
    "\n",
    "# histograms of skewed metric features\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, ceil(len(skewed_metric_feats) / 2), figsize = (22, 10))\n",
    "\n",
    "# iterate through axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), skewed_metric_feats):\n",
    "    sns.histplot(x = df[feat], ax = ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing skewed metric features before transformation for later\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# add minimum value of skewed feature to each observation to ensure all values are non-negative\n",
    "# then apply square root transformation\n",
    "for feat in skewed_metric_feats:\n",
    "    df_transformed[feat] = np.sqrt(df_transformed[feat] + abs(df_transformed[feat].min()))\n",
    "    \n",
    "# histograms of skewed metric features after sqrt transformation\n",
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, ceil(len(skewed_metric_feats) / 2), figsize = (22, 10))\n",
    "\n",
    "# iterate through axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), skewed_metric_feats):\n",
    "    sns.histplot(x = df_transformed[feat], ax = ax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features in the dataframe\n",
    "features = list(df_transformed.columns)\n",
    "\n",
    "# scaled version of the main dataframe\n",
    "df_scaled = df_transformed.copy()\n",
    "scaler = StandardScaler()\n",
    "df_scaled[features] = scaler.fit_transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7517422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point there are 3 important dataframes\n",
    "\n",
    "# dataframe containing the outliers\n",
    "# exists in case we want to assign the outliers to clusters after the model is built\n",
    "df_outliers.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main dataframe\n",
    "# used after obtaining cluster to make interpretation easier\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6440917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized dataframe\n",
    "# obtained with StandardScaler after square rooting skewed features\n",
    "# fed into a specific algorithm to obtain clusters\n",
    "df_scaled.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabde1da",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"data-clustering\"></a>\n",
    "\n",
    "***\n",
    "\n",
    "<h2><center>CLUSTERING</center></h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b76369",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering-sociodemographic\"></a>\n",
    "\n",
    "<h3><right>Sociodemographic clustering</right></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f50e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_kmeans(df, nmax_clusters):\n",
    "    \n",
    "    # store inertia values in a list\n",
    "    inertia_vals = []\n",
    "    \n",
    "    # determine inertia for each number of clusters\n",
    "    for n in np.arange(2, nmax_clusters + 1):\n",
    "        km_clusters = KMeans(n_clusters = n, random_state = 15)\n",
    "        km_clusters.fit(df)\n",
    "        inertia = km_clusters.inertia_\n",
    "        inertia_vals.append(inertia)\n",
    "    \n",
    "    # plot elbow graph\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.subplots(figsize = (8, 6))\n",
    "    sns.lineplot(x = np.arange(2, nmax_clusters + 1),\n",
    "                 y = inertia_vals,\n",
    "                 color = \"k\",\n",
    "                 marker = 'o',\n",
    "                 mew = 0,\n",
    "                 linewidth = 3)\n",
    "    plt.title(\"Elbow plot\", fontsize = 22)\n",
    "    plt.xlabel(\"Number of clusters, k\", fontsize = 18)\n",
    "    plt.ylabel(\"Inertia\", fontsize = 18)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlim([0, nmax_clusters+1])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sociodemographic features - Generation, Age, YearSal, EducDeg, Children\n",
    "# YearSal redundant with Age\n",
    "# Generation redundant with Age\n",
    "\n",
    "# dataframe with sociodemographic features for KMeans\n",
    "df_sociodem = df_scaled[[\"Age\", \"EducDeg\", \"Children\"]]\n",
    "\n",
    "df_sociodem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45378463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# elbow plot indicates ~7 clusters\n",
    "elbow_kmeans(df_sociodem, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fit KMeans model\n",
    "km1_clusters = KMeans(n_clusters = 7, random_state = 15)\n",
    "km1_clusters.fit(df_sociodem)\n",
    "\n",
    "# get cluster labels\n",
    "km1_labels = km1_clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ce92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect the differences between clusters\n",
    "\n",
    "def describe_clusters(df, labels, features_list):\n",
    "    \n",
    "    # assign a label to each row in the dataframe\n",
    "    df[\"Cluster\"] = labels\n",
    "    \n",
    "    # get summary statistics of feature(s) passed on as argument(s)\n",
    "    for feat in features_list:\n",
    "        cluster_desc = pd.DataFrame()\n",
    "        \n",
    "        for label in set(labels):\n",
    "            # compute statistics for each individual cluster\n",
    "            cluster_feat_desc = pd.Series(round(df[df.Cluster == label][feat].describe()\n",
    "                                                [[\"mean\", \"min\", \"50%\", \"max\", \"count\"]], 2),\n",
    "                                          name = \"cluster\" + str(label))\n",
    "            \n",
    "            # append statistics to cluster_desc dataframe\n",
    "            cluster_desc = cluster_desc.append(cluster_feat_desc)\n",
    "\n",
    "        print(\"=========================================\")\n",
    "        print(\"  \" + feat)\n",
    "        print(\"=========================================\")\n",
    "        print(cluster_desc.sort_values(\"mean\"))\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "describe_clusters(df, km1_labels, df_sociodem.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip plot for cluster visualization?\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# test = df.copy()\n",
    "# test.EducDeg = test.EducDeg.replace(educ_mapper)\n",
    "\n",
    "x_vars = [\"Age\", \"EducDeg\"]\n",
    "\n",
    "g = sns.PairGrid(df.sort_values(\"Age\", ascending = False),\n",
    "                 x_vars = x_vars, y_vars = [\"Cluster\"],\n",
    "                 hue = \"Children\",\n",
    "                 height = 8,\n",
    "                 aspect = 0.85)\n",
    "\n",
    "g.map(sns.stripplot, size = 12, orient = \"h\", jitter = True,\n",
    "      palette = [\"gray\", \"palevioletred\"], alpha = 0.1)\n",
    "\n",
    "# add better labels\n",
    "g.set(ylabel = \"Clusters\")\n",
    "\n",
    "titles = [\"Age\", \"EducDeg\"]\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "\n",
    "    # set axis titles\n",
    "    ax.set(title = title)\n",
    "\n",
    "    # horizontal grid\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True, lw = 1.5)\n",
    "    \n",
    "plt.legend(title = \"Children\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for cluster visualization?\n",
    "mean_cluster_ages = df.groupby(\"Cluster\").Age.mean()\n",
    "mean_cluster_educ = df.groupby(\"Cluster\").EducDeg.mean()\n",
    "mean_cluster_child = df.groupby(\"Cluster\").Children.mean()\n",
    "cluster_sizes = df.groupby(\"Cluster\").size()\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "plt.subplots(figsize = (14, 8))\n",
    "sns.scatterplot(x = mean_cluster_ages,\n",
    "               y = mean_cluster_educ,\n",
    "               hue = mean_cluster_child,\n",
    "               palette = [\"gray\", \"palevioletred\"],\n",
    "               s = cluster_sizes*10,\n",
    "               alpha = 0.8,\n",
    "               linewidth = 0)\n",
    "plt.xlim([20, 80])\n",
    "plt.xticks(np.arange(20, 81, 5))\n",
    "plt.ylim([1, 4])\n",
    "plt.yticks(np.arange(1, 5, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7723ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6314108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the remaining cluster features\n",
    "df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3f55d",
   "metadata": {},
   "source": [
    "**Summary of the sociodemographic clusters obtained**\n",
    "\n",
    "|             | C1    | C2    | C3    |          C4 |          C5 |      C6 |      C7 |\n",
    "| -----       | ----- | ----- | ----- | ----------- | ----------- | ------- | ------- |\n",
    "| **Age**     | Young | Young | Young | Middle-aged | Middle-aged | Elderly | Elderly |\n",
    "| **Educ**    | Low   | Low   | High  | Low         | High        | Low     | High    |\n",
    "| **Chilren** | Yes   | No    | Yes   | Yes         | Yes         | No      | No      |\n",
    "\n",
    "C1 - Young uneducated adults with children <br>\n",
    "C2 - Young uneducated adults without children <br>\n",
    "C3 - Young educated adults with children <br>\n",
    "C4 - Middle-aged uneducated adults with children <br>\n",
    "C5 - Middle-aged educated adults with children <br>\n",
    "C6 - Elderly with low education and no children <br>\n",
    "C7 - Elderly with high education and no children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d48ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77447b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828c0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e71d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f4ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cf62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31365aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189654ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc84ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bff554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b8a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39b06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8b9c23",
   "metadata": {},
   "source": [
    "## Sociodemographic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba283ff3",
   "metadata": {},
   "source": [
    "For sociodemographic clustering, we used the following algorithms (and features):\n",
    "1. KPrototypes (Age/Generation, EducDeg, Children)\n",
    "2. Agglomerative Clustering (Age/Generation, EducDeg, Children)\n",
    "3. Agglomerative Clustering (Age/Generation, EducDeg)\n",
    "4. KMeans (Age/Generation, EducDeg, Children)\n",
    "5. KMeans ((Age/Generation, EducDeg)\n",
    "\n",
    "Tried DBScan and Mean Shift but because they are density based the weight they give to the Children binary value makes it so that only two clusters are found. Discarding Children and using only Age and EducDeg did not lead to better solutions.\n",
    "\n",
    "Agglomerative Clustering and KMeans, because they are distance-based clustering algorithms, end up giving a lot of weight to binary variables, in this case Children. When analysing the results and the previous box plots, we concluded that while presence or absence of Children had some impact in a couple of the Premiums, that did not justify the weight they were receiving using these algorithms. For that reason, we tried both of them after discarding the feature Children. However, we felt uncomfortable losing Children as the boxplot suggests that it affects how much people pay for their Health insurance, which is the only Premium that does not appear to significantly change with EducDeg and it is not at all correlated to Age.\n",
    "\n",
    "We opted with KMeans (k = 7) as even though it attributes significant weight to children (being a binary variable), the results are similar to KPrototypes (k = 6), with the added advantage of allowing the identification of the sociodemographic group with the highest CMV by far. So overweighing children did not appear to bias the final conclusions. Plus, KPrototypes is excruciantingly slow.\n",
    "\n",
    "In KPrototypes, we tried 3 approaches:\n",
    "1. Create a high number of clusters (15) and agglomerate based on distance (as determined via a dendrogram)\n",
    "2. Create 4 clusters directly\n",
    "3. Create 6 clusters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405156e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this cell if not using hierarchical clustering\n",
    "\n",
    "def hierarchical_clusters(df, n_clusters = 2, threshold = None, affinity = \"euclidean\", linkage = \"ward\"):\n",
    "    \n",
    "    # determine clusters\n",
    "    clusters = AgglomerativeClustering(n_clusters = n_clusters,\n",
    "                                       affinity = affinity,\n",
    "                                       linkage = linkage,\n",
    "                                       distance_threshold = threshold)\n",
    "    clusters.fit(df)\n",
    "    \n",
    "    # retrieve cluster labels and distances\n",
    "    labels = clusters.labels_\n",
    "    distances = clusters.distances_\n",
    "    \n",
    "    counts = np.zeros(clusters.children_.shape[0])\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    for i, merge in enumerate(clusters.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack([clusters.children_, distances, counts]).astype(float)\n",
    "    \n",
    "    return (labels, distances, linkage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c636b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automate categorical feature detection\n",
    "\n",
    "def elbow_kprototypes(df, nmax_clusters):\n",
    "    \n",
    "    n_clusters = np.arange(2, nmax_clusters+1)\n",
    "    cost_vals = []\n",
    "    \n",
    "    for n in n_clusters:\n",
    "        kp_clusters = KPrototypes(n_clusters = n, init = \"Huang\", random_state = 15)\n",
    "        kp_clusters.fit(df, categorical = [2])\n",
    "        cost = kp_clusters.cost_\n",
    "        cost_vals.append(cost)\n",
    "        \n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    sns.lineplot(x = np.arange(2, nmax_clusters + 1), y = cost_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330836d",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KP 1 - Age, EducDeg, Children (k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp1_sociodem = pd.concat([df_scaled[[\"Age\", \"EducDeg\"]], df[\"Children\"]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62257538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow_plot(kp1_sociodem, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72288e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp1_clusters = KPrototypes(n_clusters = 4, init = \"Huang\", random_state = 15)\n",
    "#kp1_clusters.fit(kp1_sociodem, categorical = [2])\n",
    "#kp1_labels = kp1_clusters.labels_\n",
    "#kp1_centroids = kp1_clusters.cluster_centroids_\n",
    "#kp1_linkage = hierarchy.linkage(kp1_centroids, method = \"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3662cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchy.dendrogram(kp1_linkage, color_threshold = 2.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Cluster\"] = kp1_labels\n",
    "#df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2f066",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KP2 - Age, EducDeg, Children (k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp2_clusters = KPrototypes(n_clusters = 6, init = \"Huang\", random_state = 15)\n",
    "#kp2_clusters.fit(kp1_sociodem, categorical = [2])\n",
    "#kp2_labels = kp2_clusters.labels_\n",
    "#kp2_centroids = kp2_clusters.cluster_centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e62e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kp2_linkage = hierarchy.linkage(kp2_centroids, method = \"ward\")\n",
    "#hierarchy.dendrogram(kp2_linkage, color_threshold = 2.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb96290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Cluster\"] = kp2_labels\n",
    "#df.groupby(\"Cluster\").mean().sort_values(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc53aea",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "KMeans 2 - Age, EducDeg, Children\n",
    "\n",
    "KMeans + Hierarchical Clustering produced very similar results, so might as well go with this (simpler) approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
